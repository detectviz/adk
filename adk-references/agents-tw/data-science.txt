Directory structure:
└── detectviz-sre-assistant/
    └── adk-references/
        └── agents-tw/
            └── data-science/
                ├── README.md
                ├── pyproject.toml
                ├── .env.example
                ├── data_science/
                │   ├── __init__.py
                │   ├── agent.py
                │   ├── prompts.py
                │   ├── tools.py
                │   ├── sub_agents/
                │   │   ├── __init__.py
                │   │   ├── analytics/
                │   │   │   ├── __init__.py
                │   │   │   ├── agent.py
                │   │   │   └── prompts.py
                │   │   ├── bigquery/
                │   │   │   ├── __init__.py
                │   │   │   ├── agent.py
                │   │   │   ├── prompts.py
                │   │   │   ├── tools.py
                │   │   │   └── chase_sql/
                │   │   │       ├── __init__.py
                │   │   │       ├── chase_constants.py
                │   │   │       ├── chase_db_tools.py
                │   │   │       ├── dc_prompt_template.py
                │   │   │       ├── llm_utils.py
                │   │   │       ├── qp_prompt_template.py
                │   │   │       └── sql_postprocessor/
                │   │   │           ├── README.md
                │   │   │           ├── __init__.py
                │   │   │           ├── correction_prompt_template.py
                │   │   │           └── sql_translator.py
                │   │   └── bqml/
                │   │       ├── __init__.py
                │   │       ├── agent.py
                │   │       ├── prompts.py
                │   │       └── tools.py
                │   └── utils/
                │       ├── create_bq_table.py
                │       ├── reference_guide_RAG.py
                │       ├── utils.py
                │       └── data/
                │           ├── test.csv
                │           └── train.csv
                ├── deployment/
                │   ├── __init__.py
                │   ├── deploy.py
                │   └── test_deployment.py
                ├── eval/
                │   ├── __init__.py
                │   ├── test_eval.py
                │   └── eval_data/
                │       ├── simple.test.json
                │       └── test_config.json
                └── tests/
                    └── test_agents.py

================================================
FILE: adk-references/agents-tw/data-science/README.md
================================================
[Binary file]


================================================
FILE: adk-references/agents-tw/data-science/pyproject.toml
================================================
[tool.poetry]
name = "data-science"
version = "0.1"
description = "Data Science and Data QnA Multi-Agent"
authors = ["Meltem Subasioglu <msubasioglu@google.com>"]
license = "Apache License 2.0"
readme = "README.md"
package-mode = true


[tool.poetry.dependencies]
python = "^3.12"
python-dotenv = "^1.0.1"
google-adk = "^1.5.0"
immutabledict = "^4.2.1"
sqlglot = "^26.10.1"
db-dtypes = "^1.4.2"
regex = "^2024.11.6"
tabulate = "^0.9.0"
google-cloud-aiplatform = { extras = [
    "adk",
    "agent-engines",
], version = "^1.93.0" }
absl-py = "^2.2.2"
pydantic = "^2.11.3"
pandas = "^2.3.0"
numpy = "^2.3.1"

[tool.poetry.group.dev.dependencies]
google-cloud-aiplatform = { extras = [
    "adk",
    "agent-engines",
    "evaluation",
], version = "^1.93.0" }
pytest = "^8.3.5"
pytest-asyncio = "^0.26.0"
google-adk = { extras = ["eval"], version = "^1.5.0" }


[tool.pytest.ini_options]
console_output_style = "progress"
addopts = "-vv -s"
#addopts = "-vv -s --pdb"
testpaths = ["tests/"]
log_level = "DEBUG"
log_cli = true
log_auto_indent = true
log_cli_date_format = "%Y-%m-%d %H:%M:%S"
log_cli_format = "[%(asctime)s] %(levelname)s (%(funcName)s) \t [%(pathname)s:%(lineno)d] %(message)s"
filterwarnings = [
    #"error",
    "ignore::UserWarning",
    # note the use of single quote below to denote "raw" strings in TOML
    #'ignore:function ham\(\) is deprecated:DeprecationWarning',
]


[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"



================================================
FILE: adk-references/agents-tw/data-science/.env.example
================================================
# 複製為 .env 檔案並在下方填入您的值
# 執行 ./update_dotenv_example.sh 以從您的 .env 檔案更新 .env-example。

# 選擇模型後端：0 -> ML Dev, 1 -> Vertex
GOOGLE_GENAI_USE_VERTEXAI=1

# ML Dev 後端設定。若使用 ML Dev 後端請填寫。
GOOGLE_API_KEY=您的數值

# Vertex 後端設定
GOOGLE_CLOUD_PROJECT=您的數值
GOOGLE_CLOUD_LOCATION=您的數值

# SQLGen 方法
NL2SQL_METHOD="BASELINE" # BASELINE 或 CHASE

# 設定 BigQuery 代理
BQ_COMPUTE_PROJECT_ID=您的數值
BQ_DATA_PROJECT_ID=您的數值
BQ_DATASET_ID='forecasting_sticker_sales'

# 為 BQML 代理設定 RAG 語料庫
BQML_RAG_CORPUS_NAME='' # 將此留空，它將會自動填入

# 設定程式碼直譯器 (Code Interpreter)，如果存在的話。否則留空
CODE_INTERPRETER_EXTENSION_NAME='' # '' 或 'projects/{GOOGLE_CLOUD_PROJECT}/locations/us-central1/extensions/{EXTENSION_ID}'

# 代理中使用的模型
ROOT_AGENT_MODEL='gemini-1.5-flash'
ANALYTICS_AGENT_MODEL='gemini-1.5-flash'
BIGQUERY_AGENT_MODEL='gemini-1.5-flash'
BASELINE_NL2SQL_MODEL='gemini-1.5-flash'
CHASE_NL2SQL_MODEL='gemini-1.5-flash'
BQML_AGENT_MODEL='gemini-1.5-flash'



================================================
FILE: adk-references/agents-tw/data-science/data_science/__init__.py
================================================
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os

from . import agent

__all__ = ["agent"]



================================================
FILE: adk-references/agents-tw/data-science/data_science/agent.py
================================================
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""資料代理多代理系統的頂層代理。

-- 它使用 NL2SQL 從資料庫（例如 BigQuery）取得資料
-- 然後，它視需要使用 NL2Py 進行進一步的資料分析
"""
import os
from datetime import date

from google.genai import types

from google.adk.agents import Agent
from google.adk.agents.callback_context import CallbackContext
from google.adk.tools import load_artifacts

from .sub_agents import bqml_agent
from .sub_agents.bigquery.tools import (
    get_database_settings as get_bq_database_settings,
)
from .prompts import return_instructions_root
from .tools import call_db_agent, call_ds_agent

date_today = date.today()


def setup_before_agent_call(callback_context: CallbackContext):
    """設定代理。"""

    # 在 session.state 中設定資料庫設定
    if "database_settings" not in callback_context.state:
        db_settings = dict()
        db_settings["use_database"] = "BigQuery"
        callback_context.state["all_db_settings"] = db_settings

    # 在指令中設定結構 (schema)
    if callback_context.state["all_db_settings"]["use_database"] == "BigQuery":
        callback_context.state["database_settings"] = get_bq_database_settings()
        schema = callback_context.state["database_settings"]["bq_ddl_schema"]

        callback_context._invocation_context.agent.instruction = (
            return_instructions_root()
            + f"""

    --------- 相關資料的 BigQuery 結構 (schema) 及幾筆範例資料列。 ---------
    {schema}

    """
        )


root_agent = Agent(
    model=os.getenv("ROOT_AGENT_MODEL"),
    name="db_ds_multiagent",
    instruction=return_instructions_root(),
    global_instruction=(
        f"""
        您是一個資料科學與資料分析多代理 (Multi-Agent) 系統。
        今天的日期：{date_today}
        """
    ),
    sub_agents=[bqml_agent],
    tools=[
        call_db_agent,
        call_ds_agent,
        load_artifacts,
    ],
    before_agent_callback=setup_before_agent_call,
    generate_content_config=types.GenerateContentConfig(temperature=0.01),
)



================================================
FILE: adk-references/agents-tw/data-science/data_science/prompts.py
================================================
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""儲存和擷取代理指令的模組。

此模組定義了回傳給根代理 (root agent) 的指令提示詞 (prompts) 的函式。
這些指令引導代理的行為、工作流程和工具使用。
"""


def return_instructions_root() -> str:

    instruction_prompt_root_v2 = """

    您是一位資深的資料科學家，任務是準確地分類使用者關於特定資料庫的意圖，並針對該資料庫制定適合 SQL 資料庫代理 (`call_db_agent`) 和 Python 資料科學代理 (`call_ds_agent`) 的具體問題（如有必要）。
    - 資料代理可以存取下面指定的資料庫。
    - 如果使用者提出的問題可以直接從資料庫結構 (schema) 中得到答案，請直接回答，無需呼叫任何其他代理。
    - 如果問題是超出資料庫存取範圍的複合問題，例如執行資料分析或預測性建模，請將問題改寫為兩部分：1) 需要 SQL 執行的部分和 2) 需要 Python 分析的部分。視需要呼叫資料庫代理和/或資料科學代理。
    - 如果問題需要執行 SQL，請將其轉發給資料庫代理。
    - 如果問題需要執行 SQL 並進行額外分析，請將其轉發給資料庫代理和資料科學代理。
    - 如果使用者特別指定要使用 BQML，請路由至 bqml_agent。

    - 重要提示：請務必精確！如果使用者要求提供資料集，請提供名稱。若非絕對必要，請勿呼叫任何額外的代理！

    <TASK>

        # **工作流程：**

        # 1. **理解意圖**

        # 2. **擷取資料工具 (`call_db_agent` - 如適用)：** 如果您需要查詢資料庫，請使用此工具。請確保提供適當的查詢以完成任務。

        # 3. **分析資料工具 (`call_ds_agent` - 如適用)：** 如果您需要執行資料科學任務和 python 分析，請使用此工具。請確保提供適當的查詢以完成任務。

        # 4a. **BigQuery ML 工具 (`call_bqml_agent` - 如適用)：** 如果使用者特別要求 (!) 使用 BigQuery ML，請使用此工具。請確保提供適當的查詢、資料集和專案 ID 以及上下文以完成任務。

        # 5. **回應：** 回傳 `RESULT` 和 `EXPLANATION`，如果有圖表，則可選回傳 `GRAPH`。請使用 MARKDOWN 格式（而非 JSON），並包含以下部分：

        #     * **結果：** "資料代理發現的自然語言摘要"

        #     * **說明：** "結果是如何得出的逐步說明。",

        # **工具使用摘要：**

        #   * **問候/超出範圍：** 直接回答。
        #   * **SQL 查詢：** `call_db_agent`。回傳答案後，提供額外說明。
        #   * **SQL & Python 分析：** `call_db_agent`，然後是 `call_ds_agent`。回傳答案後，提供額外說明。
        #   * **BQ ML `call_bqml_agent`：** 如果使用者要求，查詢 BQ ML 代理。確保：
        #   A. 您提供合適的查詢。
        #   B. 您傳遞專案和資料集 ID。
        #   C. 您傳遞任何額外的上下文。


        **關鍵提醒：**
        * ** 您可以存取資料庫結構！不要向資料庫代理詢問結構，請先使用您自己的資訊！**
        * **絕不產生 SQL 程式碼。那不是您的任務。請改用工具。
        * **僅在使用者特別要求 BQML / BIGQUERY ML 時才呼叫 BQML 代理。這適用於任何 BQML 相關任務，例如檢查模型、訓練、推論等。**
        * **不要產生 python 程式碼，如果需要進一步分析，請務必使用 call_ds_agent。**
        * **不要產生 SQL 程式碼，如果需要，請務必使用 call_db_agent 來產生 SQL。**
        * **如果 call_ds_agent 呼叫成功並回傳有效結果，只需使用回應格式總結先前所有步驟的結果！**
        * **如果先前的 call_db_agent 和 call_ds_agent 已提供資料，您可以直接使用 call_ds_agent，利用先前步驟的資料進行新的分析**
        * **不要向使用者詢問專案或資料集 ID。您可以在會話上下文中找到這些詳細資訊。對於 BQ ML 任務，只需確認是否可以繼續執行計畫。**
    </TASK>


    <CONSTRAINTS>
        * **遵守結構 (Schema Adherence)：** **嚴格遵守提供的結構。** 不要發明或假設任何超出給定範圍的資料或結構元素。
        * **優先考慮清晰度：** 如果使用者的意圖過於廣泛或模糊（例如，詢問「資料」卻沒有具體說明），請優先選擇 **問候/能力** 回應，並根據結構提供可用資料的清晰描述。
    </CONSTRAINTS>

    """

    instruction_prompt_root_v1 = """您是一位使用所提供工具回答資料相關問題的 AI 助理。
    您的任務是準確分類使用者的意圖，並制定適合以下代理的精煉問題：
    - SQL 資料庫代理 (`call_db_agent`)
    - Python 資料科學代理 (`call_ds_agent`)
    - BigQuery ML 代理 (`call_bqml_agent`)（如有必要）。


    # **工作流程：**

    # 1. **理解意圖工具 (`call_intent_understanding`)：** 此工具對使用者問題進行分類，並回傳具有四種結構之一的 JSON：

    #     * **問候 (Greeting)：** 包含 `greeting_message`。直接回傳此訊息。
    #     * **使用資料庫 (Use Database)：** (可選) 包含 `use_database`。用此來決定要使用哪個資料庫。回傳我們切換到 XXX 資料庫。
    #     * **超出範圍 (Out of Scope)：** 回傳：「您的問題超出了此資料庫的範圍。請提出與此資料庫相關的問題。」
    #     * **僅 SQL 查詢 (SQL Query Only)：** 包含 `nl_to_sql_question`。繼續執行步驟 2。
    #     * **SQL 與 Python 分析 (SQL and Python Analysis)：** 包含 `nl_to_sql_question` 和 `nl_to_python_question`。繼續執行步驟 2。


    # 2. **擷取資料工具 (`call_db_agent` - 如適用)：** 如果您需要查詢資料庫，請使用此工具。請確保提供適當的查詢以完成任務。

    # 3. **分析資料工具 (`call_ds_agent` - 如適用)：** 如果您需要執行資料科學任務和 python 分析，請使用此工具。請確保提供適當的查詢以完成任務。

    # 4a. **BigQuery ML 工具 (`call_bqml_agent` - 如適用)：** 如果使用者特別要求 (!) 使用 BigQuery ML，請使用此工具。請確保提供適當的查詢、資料集和專案 ID 以及上下文以完成任務。

    # 5. **回應：** 回傳 `RESULT` 和 `EXPLANATION`，如果有圖表，則可選回傳 `GRAPH`。請使用 MARKDOWN 格式（而非 JSON），並包含以下部分：

    #     * **結果：** "資料代理發現的自然語言摘要"

    #     * **說明：** "結果是如何得出的逐步說明。",

    # **工具使用摘要：**

    #   * **問候/超出範圍：** 直接回答。
    #   * **SQL 查詢：** `call_db_agent`。回傳答案後，提供額外說明。
    #   * **SQL & Python 分析：** `call_db_agent`，然後是 `call_ds_agent`。回傳答案後，提供額外說明。
    #   * **BQ ML `call_bqml_agent`：** 如果使用者要求，查詢 BQ ML 代理。確保：
    #   A. 您提供合適的查詢。
    #   B. 您傳遞專案和資料集 ID。
    #   C. 您傳遞任何額外的上下文。


    **關鍵提醒：**
    * ** 您可以存取資料庫結構。請使用它。**
    * **僅在使用者特別要求 BQML / BIGQUERY ML 時才呼叫 BQML 代理。這適用於任何 BQML 相關任務，例如檢查模型、訓練、推論等。**
    * **不要產生 python 程式碼，如果需要進一步分析，請務必使用 call_ds_agent。**
    * **不要產生 SQL 程式碼，如果需要，請務必使用 call_db_agent 來產生 SQL。**
    * **如果 call_ds_agent 呼叫成功並回傳有效結果，只需使用回應格式總結先前所有步驟的結果！**
    * **如果先前的 call_db_agent 和 call_ds_agent 已提供資料，您可以直接使用 call_ds_agent，利用先前步驟的資料進行新的分析，跳過 call_intent_understanding 和 call_db_agent！**
    * **不要向使用者詢問專案或資料集 ID。您可以在會話上下文中找到這些詳細資訊。對於 BQ ML 任務，只需確認是否可以繼續執行計畫。**
        """

    instruction_prompt_root_v0 = """您是一位使用所提供工具回答資料相關問題的 AI 助理。


        **工作流程：**

        1. **理解意圖工具 (`call_intent_understanding`)：** 此工具對使用者問題進行分類，並回傳具有四種結構之一的 JSON：

            * **問候 (Greeting)：** 包含 `greeting_message`。直接回傳此訊息。
            * **使用資料庫 (Use Database)：** (可選) 包含 `use_database`。用此來決定要使用哪個資料庫。回傳我們切換到 XXX 資料庫。
            * **超出範圍 (Out of Scope)：** 回傳：「您的問題超出了此資料庫的範圍。請提出與此資料庫相關的問題。」
            * **僅 SQL 查詢 (SQL Query Only)：** 包含 `nl_to_sql_question`。繼續執行步驟 2。
            * **SQL 與 Python 分析 (SQL and Python Analysis)：** 包含 `nl_to_sql_question` 和 `nl_to_python_question`。繼續執行步驟 2。


        2. **擷取資料工具 (`call_db_agent` - 如適用)：** 如果您需要查詢資料庫，請使用此工具。請確保提供適當的查詢以完成任務。

        3. **分析資料工具 (`call_ds_agent` - 如適用)：** 如果您需要執行資料科學任務和 python 分析，請使用此工具。請確保提供適當的查詢以完成任務。

        4a. **BigQuery ML 工具 (`call_bqml_agent` - 如適用)：** 如果使用者特別要求 (!) 使用 BigQuery ML，請使用此工具。請確保提供適當的查詢、資料集和專案 ID 以及上下文以完成任務。完成後，請與使用者確認計畫再繼續。
            如果使用者接受計畫，請再次呼叫此工具以執行。


        5. **回應：** 回傳 `RESULT` 和 `EXPLANATION`，如果有圖表，則可選回傳 `GRAPH`。請使用 MARKDOWN 格式（而非 JSON），並包含以下部分：

            * **結果：** "資料代理發現的自然語言摘要"

            * **說明：** "結果是如何得出的逐步說明。",

        **工具使用摘要：**

        * **問候/超出範圍：** 直接回答。
        * **SQL 查詢：** `call_db_agent`。回傳答案後，提供額外說明。
        * **SQL & Python 分析：** `call_db_agent`，然後是 `call_ds_agent`。回傳答案後，提供額外說明。
        * **BQ ML `call_bqml_agent`：** 如果使用者要求，查詢 BQ ML 代理。確保：
        A. 您提供合適的查詢。
        B. 您傳遞專案和資料集 ID。
        C. 您傳遞任何額外的上下文。

        **關鍵提醒：**
        * **不要捏造任何答案。完全依賴所提供的工具。如果不確定，請務必先使用 call_intent_understanding！**
        * **不要產生 python 程式碼，如果 nl_to_python_question 不是 N/A，請務必使用 call_ds_agent 進行進一步分析！**
        * **如果 call_ds_agent 呼叫成功並回傳有效結果，只需使用回應格式總結先前所有步驟的結果！**
        * **如果先前的 call_db_agent 和 call_ds_agent 已提供資料，您可以直接使用 call_ds_agent，利用先前步驟的資料進行新的分析，跳過 call_intent_understanding 和 call_db_agent！**
        * **絕不直接產生答案；對於任何問題，請務必使用給定的工具。如果不確定，請從 call_intent_understanding 開始！**
            """

    return instruction_prompt_root_v2



================================================
FILE: adk-references/agents-tw/data-science/data_science/tools.py
================================================
[Binary file]


================================================
FILE: adk-references/agents-tw/data-science/data_science/sub_agents/__init__.py
================================================
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from .bqml.agent import root_agent as bqml_agent
from .analytics.agent import root_agent as ds_agent
from .bigquery.agent import database_agent as db_agent


__all__ = ["bqml_agent", "ds_agent", "db_agent"]



================================================
FILE: adk-references/agents-tw/data-science/data_science/sub_agents/analytics/__init__.py
================================================
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.




================================================
FILE: adk-references/agents-tw/data-science/data_science/sub_agents/analytics/agent.py
================================================
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""資料科學代理 V2：產生 nl2py 並使用程式碼直譯器 (Code Interpreter) 執行程式碼。"""
import os
from google.adk.code_executors import VertexAiCodeExecutor
from google.adk.agents import Agent
from .prompts import return_instructions_ds


root_agent = Agent(
    model=os.getenv("ANALYTICS_AGENT_MODEL"),
    name="data_science_agent",
    instruction=return_instructions_ds(),
    code_executor=VertexAiCodeExecutor(
        optimize_data_file=True,
        stateful=True,
    ),
)



================================================
FILE: adk-references/agents-tw/data-science/data_science/sub_agents/analytics/prompts.py
================================================
[Binary file]


================================================
FILE: adk-references/agents-tw/data-science/data_science/sub_agents/bigquery/__init__.py
================================================
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.




================================================
FILE: adk-references/agents-tw/data-science/data_science/sub_agents/bigquery/agent.py
================================================
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""資料庫代理：使用 NL2SQL 從資料庫 (BigQuery) 取得資料。"""

import os

from google.adk.agents import Agent
from google.adk.agents.callback_context import CallbackContext
from google.genai import types

from . import tools
from .chase_sql import chase_db_tools
from .prompts import return_instructions_bigquery

NL2SQL_METHOD = os.getenv("NL2SQL_METHOD", "BASELINE")


def setup_before_agent_call(callback_context: CallbackContext) -> None:
    """設定代理。"""

    if "database_settings" not in callback_context.state:
        callback_context.state["database_settings"] = \
            tools.get_database_settings()


database_agent = Agent(
    model=os.getenv("BIGQUERY_AGENT_MODEL"),
    name="database_agent",
    instruction=return_instructions_bigquery(),
    tools=[
        (
            chase_db_tools.initial_bq_nl2sql
            if NL2SQL_METHOD == "CHASE"
            else tools.initial_bq_nl2sql
        ),
        tools.run_bigquery_validation,
    ],
    before_agent_callback=setup_before_agent_call,
    generate_content_config=types.GenerateContentConfig(temperature=0.01),
)



================================================
FILE: adk-references/agents-tw/data-science/data_science/sub_agents/bigquery/prompts.py
================================================
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""儲存和擷取代理指令的模組。

此模組定義了回傳給 BigQuery 代理的指令提示詞 (prompts) 的函式。
這些指令引導代理的行為、工作流程和工具使用。
"""

import os


def return_instructions_bigquery() -> str:

    NL2SQL_METHOD = os.getenv("NL2SQL_METHOD", "BASELINE")
    if NL2SQL_METHOD == "BASELINE" or NL2SQL_METHOD == "CHASE":
        db_tool_name = "initial_bq_nl2sql"
    else:
        db_tool_name = None
        raise ValueError(f"未知的 NL2SQL 方法：{NL2SQL_METHOD}")

    instruction_prompt_bqml_v1 = f"""
      您是一位擔任 BigQuery 的 SQL 專家的 AI 助理。
      您的工作是幫助使用者從自然語言問題（在 Nl2sqlInput 內）產生 SQL 答案。
      您應該以 NL2SQLOutput 的格式產生結果。

      使用提供的工具來幫助產生最準確的 SQL：
      1. 首先，使用 {db_tool_name} 工具從問題中產生初始的 SQL。
      2. 您還應該驗證您建立的 SQL 是否有語法和函式錯誤（使用 run_bigquery_validation 工具）。如果有任何錯誤，您應該返回並修正 SQL 中的錯誤。透過修正錯誤來重新建立 SQL。
      4. 以 JSON 格式產生最終結果，包含四個鍵值： "explain"、"sql"、"sql_results"、"nl_results"。
          "explain": "寫出逐步的推理過程，解釋您是如何根據結構、範例和問題來產生查詢的。",
          "sql": "輸出您產生的 SQL！",
          "sql_results": "如果 run_bigquery_validation 有可用的原始 sql 執行 query_result，否則為 None",
          "nl_results": "關於結果的自然語言，如果產生的 SQL 無效，則為 None"
      ```
      您應該視需要將一個工具呼叫傳遞給另一個工具呼叫！

      注意：您應該永遠使用工具 ({db_tool_name} 和 run_bigquery_validation) 來產生 SQL，而不是在不呼叫工具的情況下自行編造 SQL。
      請記住，您是一個協調代理，而不是 SQL 專家，所以請使用工具來幫助您產生 SQL，但不要自行編造 SQL。

    """

    return instruction_prompt_bqml_v1



================================================
FILE: adk-references/agents-tw/data-science/data_science/sub_agents/bigquery/tools.py
================================================
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""This file contains the tools used by the database agent."""

import datetime
import logging
import os
import re

import numpy as np
import pandas as pd
from data_science.utils.utils import get_env_var
from google.adk.tools import ToolContext
from google.cloud import bigquery
from google.genai import Client

from .chase_sql import chase_constants

# Assume that `BQ_COMPUTE_PROJECT_ID` and `BQ_DATA_PROJECT_ID` are set in the
# environment. See the `data_agent` README for more details.
data_project = os.getenv("BQ_DATA_PROJECT_ID", None)
compute_project = os.getenv("BQ_COMPUTE_PROJECT_ID", None)
vertex_project = os.getenv("GOOGLE_CLOUD_PROJECT", None)
location = os.getenv("GOOGLE_CLOUD_LOCATION", "us-central1")
llm_client = Client(vertexai=True, project=vertex_project, location=location)

MAX_NUM_ROWS = 80


def _serialize_value_for_sql(value):
    """Serializes a Python value from a pandas DataFrame into a BigQuery SQL literal."""
    if pd.isna(value):
        return "NULL"
    if isinstance(value, str):
        # Escape single quotes and backslashes for SQL strings.
        return f"'{value.replace('\\', '\\\\').replace("'", "''")}'"
    if isinstance(value, bytes):
        return f"b'{value.decode('utf-8', 'replace').replace('\\', '\\\\').replace("'", "''")}'"
    if isinstance(value, (datetime.datetime, datetime.date, pd.Timestamp)):
        # Timestamps and datetimes need to be quoted.
        return f"'{value}'"
    if isinstance(value, (list, np.ndarray)):
        # Format arrays.
        return f"[{', '.join(_serialize_value_for_sql(v) for v in value)}]"
    if isinstance(value, dict):
        # For STRUCT, BQ expects ('val1', 'val2', ...).
        # The values() order from the dataframe should match the column order.
        return f"({', '.join(_serialize_value_for_sql(v) for v in value.values())})"
    return str(value)


database_settings = None
bq_client = None


def get_bq_client():
    """Get BigQuery client."""
    global bq_client
    if bq_client is None:
        bq_client = bigquery.Client(
            project=get_env_var("BQ_COMPUTE_PROJECT_ID"))
    return bq_client


def get_database_settings():
    """Get database settings."""
    global database_settings
    if database_settings is None:
        database_settings = update_database_settings()
    return database_settings


def update_database_settings():
    """Update database settings."""
    global database_settings
    ddl_schema = get_bigquery_schema(
        dataset_id=get_env_var("BQ_DATASET_ID"),
        data_project_id=get_env_var("BQ_DATA_PROJECT_ID"),
        client=get_bq_client(),
        compute_project_id=get_env_var("BQ_COMPUTE_PROJECT_ID")
    )
    database_settings = {
        "bq_project_id": get_env_var("BQ_DATA_PROJECT_ID"),
        "bq_dataset_id": get_env_var("BQ_DATASET_ID"),
        "bq_ddl_schema": ddl_schema,
        # Include ChaseSQL-specific constants.
        **chase_constants.chase_sql_constants_dict,
    }
    return database_settings


def get_bigquery_schema(dataset_id,
                        data_project_id,
                        client=None,
                        compute_project_id=None):
    """Retrieves schema and generates DDL with example values for a BigQuery dataset.

    Args:
        dataset_id (str): The ID of the BigQuery dataset (e.g., 'my_dataset').
        data_project_id (str): Project used for BQ data.
        client (bigquery.Client): A BigQuery client.
        compute_project_id (str): Project used for BQ compute.

    Returns:
        str: A string containing the generated DDL statements.
    """

    if client is None:
        client = bigquery.Client(project=compute_project_id)

    # dataset_ref = client.dataset(dataset_id)
    dataset_ref = bigquery.DatasetReference(data_project_id, dataset_id)

    ddl_statements = ""

    # Query INFORMATION_SCHEMA to robustly list tables. This is the recommended
    # approach when a dataset may contain BigLake tables like Apache Iceberg,
    # as the tables.list API can fail in those cases.
    info_schema_query = f"""
        SELECT table_name
        FROM `{data_project_id}.{dataset_id}.INFORMATION_SCHEMA.TABLES`
    """
    query_job = client.query(info_schema_query)

    for table_row in query_job.result():
        table_ref = dataset_ref.table(table_row.table_name)
        table_obj = client.get_table(table_ref)

        if table_obj.table_type == "VIEW":
            view_query = table_obj.view_query
            ddl_statements += (
                f"CREATE OR REPLACE VIEW `{table_ref}` AS\n{view_query};\n\n"
            )
            continue
        elif table_obj.table_type == "EXTERNAL":
            if (
                table_obj.external_data_configuration
                and table_obj.external_data_configuration.source_format
                == "ICEBERG"
            ):
                config = table_obj.external_data_configuration
                uris_list_str = ",\n    ".join(
                    [f"'{uri}'" for uri in config.source_uris]
                )

                # Build column definitions from schema
                column_defs = []
                for field in table_obj.schema:
                    col_type = field.field_type
                    if field.mode == "REPEATED":
                        col_type = f"ARRAY<{col_type}>"
                    column_defs.append(f"  `{field.name}` {col_type}")
                columns_str = ",\n".join(column_defs)

                ddl_statements += f"""CREATE EXTERNAL TABLE `{table_ref}` (
{columns_str}
)
WITH CONNECTION `{config.connection_id}`
OPTIONS (
  uris = [{uris_list_str}],
  format = 'ICEBERG'
);\n\n"""
            # Skip DDL generation for other external tables.
            continue
        elif table_obj.table_type == "TABLE":
            column_defs = []
            for field in table_obj.schema:
                col_type = field.field_type
                if field.mode == "REPEATED":
                    col_type = f"ARRAY<{col_type}>"
                col_def = f"  `{field.name}` {col_type}"
                if field.description:
                    # Use OPTIONS for column descriptions
                    col_def += (
                        " OPTIONS(description='"
                        f"{field.description.replace("'", "''")}')"
                    )
                column_defs.append(col_def)

            ddl_statement = (
                f"CREATE OR REPLACE TABLE `{table_ref}` "
                f"(\n{',\n'.join(column_defs)}\n);\n\n"
            )

            # Add example values if available by running a query. This is more
            # robust than list_rows, especially for BigLake tables like Iceberg.
            try:
                sample_query = f"SELECT * FROM `{table_ref}` LIMIT 5"
                rows = client.query(sample_query).to_dataframe()

                if not rows.empty:
                    ddl_statement += f"-- Example values for table `{table_ref}`:\n"
                    for _, row in rows.iterrows():
                        values_str = ", ".join(
                            _serialize_value_for_sql(v) for v in row.values
                        )
                        ddl_statement += (
                            f"INSERT INTO `{table_ref}` VALUES ({values_str});\n\n"
                        )
            except Exception as e:
                logging.warning(
                    f"Could not retrieve sample rows for table {table_ref.path}: {e}"
                )
                ddl_statement += f"-- NOTE: Could not retrieve sample rows for table {table_ref.path}.\n\n"

            ddl_statements += ddl_statement
        else:
            # Skip other types like MATERIALIZED_VIEW, SNAPSHOT etc.
            continue

    return ddl_statements


def initial_bq_nl2sql(
    question: str,
    tool_context: ToolContext,
) -> str:
    """Generates an initial SQL query from a natural language question.

    Args:
        question (str): Natural language question.
        tool_context (ToolContext): The tool context to use for generating the SQL
          query.

    Returns:
        str: An SQL statement to answer this question.
    """

    prompt_template = """
You are a BigQuery SQL expert tasked with answering user's questions about BigQuery tables by generating SQL queries in the GoogleSql dialect.  Your task is to write a Bigquery SQL query that answers the following question while using the provided context.

**Guidelines:**

- **Table Referencing:** Always use the full table name with the database prefix in the SQL statement.  Tables should be referred to using a fully qualified name with enclosed in backticks (`) e.g. `project_name.dataset_name.table_name`.  Table names are case sensitive.
- **Joins:** Join as few tables as possible. When joining tables, ensure all join columns are the same data type. Analyze the database and the table schema provided to understand the relationships between columns and tables.
- **Aggregations:**  Use all non-aggregated columns from the `SELECT` statement in the `GROUP BY` clause.
- **SQL Syntax:** Return syntactically and semantically correct SQL for BigQuery with proper relation mapping (i.e., project_id, owner, table, and column relation). Use SQL `AS` statement to assign a new name temporarily to a table column or even a table wherever needed. Always enclose subqueries and union queries in parentheses.
- **Column Usage:** Use *ONLY* the column names (column_name) mentioned in the Table Schema. Do *NOT* use any other column names. Associate `column_name` mentioned in the Table Schema only to the `table_name` specified under Table Schema.
- **FILTERS:** You should write query effectively  to reduce and minimize the total rows to be returned. For example, you can use filters (like `WHERE`, `HAVING`, etc. (like 'COUNT', 'SUM', etc.) in the SQL query.
- **LIMIT ROWS:**  The maximum number of rows returned should be less than {MAX_NUM_ROWS}.

**Schema:**

The database structure is defined by the following table schemas (possibly with sample rows):

```
{SCHEMA}
```

**Natural language question:**

```
{QUESTION}
```

**Think Step-by-Step:** Carefully consider the schema, question, guidelines, and best practices outlined above to generate the correct BigQuery SQL.

   """

    ddl_schema = tool_context.state["database_settings"]["bq_ddl_schema"]

    prompt = prompt_template.format(
        MAX_NUM_ROWS=MAX_NUM_ROWS, SCHEMA=ddl_schema, QUESTION=question
    )

    response = llm_client.models.generate_content(
        model=os.getenv("BASELINE_NL2SQL_MODEL"),
        contents=prompt,
        config={"temperature": 0.1},
    )

    sql = response.text
    if sql:
        sql = sql.replace("```sql", "").replace("```", "").strip()

    print("\n sql:", sql)

    tool_context.state["sql_query"] = sql

    return sql


def run_bigquery_validation(
    sql_string: str,
    tool_context: ToolContext,
) -> str:
    """Validates BigQuery SQL syntax and functionality.

    This function validates the provided SQL string by attempting to execute it
    against BigQuery in dry-run mode. It performs the following checks:

    1. **SQL Cleanup:**  Preprocesses the SQL string using a `cleanup_sql`
    function
    2. **DML/DDL Restriction:**  Rejects any SQL queries containing DML or DDL
       statements (e.g., UPDATE, DELETE, INSERT, CREATE, ALTER) to ensure
       read-only operations.
    3. **Syntax and Execution:** Sends the cleaned SQL to BigQuery for validation.
       If the query is syntactically correct and executable, it retrieves the
       results.
    4. **Result Analysis:**  Checks if the query produced any results. If so, it
       formats the first few rows of the result set for inspection.

    Args:
        sql_string (str): The SQL query string to validate.
        tool_context (ToolContext): The tool context to use for validation.

    Returns:
        str: A message indicating the validation outcome. This includes:
             - "Valid SQL. Results: ..." if the query is valid and returns data.
             - "Valid SQL. Query executed successfully (no results)." if the query
                is valid but returns no data.
             - "Invalid SQL: ..." if the query is invalid, along with the error
                message from BigQuery.
    """

    def cleanup_sql(sql_string):
        """Processes the SQL string to get a printable, valid SQL string."""

        # 1. Remove backslashes escaping double quotes
        sql_string = sql_string.replace('\\"', '"')

        # 2. Remove backslashes before newlines (the key fix for this issue)
        sql_string = sql_string.replace("\\\n", "\n")  # Corrected regex

        # 3. Replace escaped single quotes
        sql_string = sql_string.replace("\\'", "'")

        # 4. Replace escaped newlines (those not preceded by a backslash)
        sql_string = sql_string.replace("\\n", "\n")

        # 5. Add limit clause if not present
        if "limit" not in sql_string.lower():
            sql_string = sql_string + " limit " + str(MAX_NUM_ROWS)

        return sql_string

    logging.info("Validating SQL: %s", sql_string)
    sql_string = cleanup_sql(sql_string)
    logging.info("Validating SQL (after cleanup): %s", sql_string)

    final_result = {"query_result": None, "error_message": None}

    # More restrictive check for BigQuery - disallow DML and DDL
    if re.search(
        r"(?i)(update|delete|drop|insert|create|alter|truncate|merge)", sql_string
    ):
        final_result["error_message"] = (
            "Invalid SQL: Contains disallowed DML/DDL operations."
        )
        return final_result

    try:
        query_job = get_bq_client().query(sql_string)
        results = query_job.result()  # Get the query results

        if results.schema:  # Check if query returned data
            rows = [
                {
                    key: (
                        value
                        if not isinstance(value, datetime.date)
                        else value.strftime("%Y-%m-%d")
                    )
                    for (key, value) in row.items()
                }
                for row in results
            ][
                :MAX_NUM_ROWS
            ]  # Convert BigQuery RowIterator to list of dicts
            # return f"Valid SQL. Results: {rows}"
            final_result["query_result"] = rows

            tool_context.state["query_result"] = rows

        else:
            final_result["error_message"] = (
                "Valid SQL. Query executed successfully (no results)."
            )

    except (
        Exception
    ) as e:  # Catch generic exceptions from BigQuery  # pylint: disable=broad-exception-caught
        final_result["error_message"] = f"Invalid SQL: {e}"

    print("\n run_bigquery_validation final_result: \n", final_result)

    return final_result



================================================
FILE: adk-references/agents-tw/data-science/data_science/sub_agents/bigquery/chase_sql/__init__.py
================================================
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.




================================================
FILE: adk-references/agents-tw/data-science/data_science/sub_agents/bigquery/chase_sql/chase_constants.py
================================================
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Constants used by the ChaseSQL algorithm."""
import os
from typing import Any
import immutabledict


# Parameters for ChaseSQL.
chase_sql_constants_dict: immutabledict.immutabledict[str, Any] = (
    immutabledict.immutabledict(
        {
            # Whether to transpile the SQL to BigQuery.
            "transpile_to_bigquery": True,
            # Whether to process input errors.
            "process_input_errors": True,
            # Whether to process SQLGlot tool output errors.
            "process_tool_output_errors": True,
            # Number of candidates to generate.
            "number_of_candidates": 1,
            # Model to use for generation.
            "model": os.getenv("CHASE_NL2SQL_MODEL"),
            # Temperature for generation.
            "temperature": 0.5,
            # Type of SQL generation method.
            "generate_sql_type": "dc",
        }
    )
)



================================================
FILE: adk-references/agents-tw/data-science/data_science/sub_agents/bigquery/chase_sql/chase_db_tools.py
================================================
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""This code contains the implementation of the tools used for the CHASE-SQL agent."""

import enum
import os

from google.adk.tools import ToolContext

# pylint: disable=g-importing-member
from .dc_prompt_template import DC_PROMPT_TEMPLATE
from .llm_utils import GeminiModel
from .qp_prompt_template import QP_PROMPT_TEMPLATE
from .sql_postprocessor import sql_translator

# pylint: enable=g-importing-member

BQ_DATA_PROJECT_ID = os.getenv("BQ_DATA_PROJECT_ID")


class GenerateSQLType(enum.Enum):
    """Enum for the different types of SQL generation methods.

    DC: Divide and Conquer ICL prompting
    QP: Query Plan-based prompting
    """

    DC = "dc"
    QP = "qp"


def exception_wrapper(func):
    """A decorator to catch exceptions in a function and return the exception as a string.

    Args:
       func (callable): The function to wrap.

    Returns:
       callable: The wrapped function.
    """

    def wrapped_function(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:  # pylint: disable=broad-exception-caught
            return f"Exception occurred in {func.__name__}: {str(e)}"

    return wrapped_function


def parse_response(response: str) -> str:
    """Parses the output to extract SQL content from the response.

    Args:
       response (str): The output string containing SQL query.

    Returns:
       str: The SQL query extracted from the response.
    """
    query = response
    try:
        if "```sql" in response and "```" in response:
            query = response.split("```sql")[1].split("```")[0]
    except ValueError as e:
        print(f"Error in parsing response: {e}")
        query = response
    return query.strip()


def initial_bq_nl2sql(
    question: str,
    tool_context: ToolContext,
) -> str:
    """Generates an initial SQL query from a natural language question.

    Args:
      question: Natural language question.
      tool_context: Function context.

    Returns:
      str: An SQL statement to answer this question.
    """
    print("****** Running agent with ChaseSQL algorithm.")
    ddl_schema = tool_context.state["database_settings"]["bq_ddl_schema"]
    project = tool_context.state["database_settings"]["bq_data_project_id"]
    db = tool_context.state["database_settings"]["bq_dataset_id"]
    transpile_to_bigquery = tool_context.state["database_settings"][
        "transpile_to_bigquery"
    ]
    process_input_errors = tool_context.state["database_settings"][
        "process_input_errors"
    ]
    process_tool_output_errors = tool_context.state["database_settings"][
        "process_tool_output_errors"
    ]
    number_of_candidates = tool_context.state["database_settings"][
        "number_of_candidates"
    ]
    model = tool_context.state["database_settings"]["model"]
    temperature = tool_context.state["database_settings"]["temperature"]
    generate_sql_type = tool_context.state["database_settings"]["generate_sql_type"]

    if generate_sql_type == GenerateSQLType.DC.value:
        prompt = DC_PROMPT_TEMPLATE.format(
            SCHEMA=ddl_schema,
            QUESTION=question,
            BQ_DATA_PROJECT_ID=BQ_DATA_PROJECT_ID
        )
    elif generate_sql_type == GenerateSQLType.QP.value:
        prompt = QP_PROMPT_TEMPLATE.format(
            SCHEMA=ddl_schema,
            QUESTION=question,
            BQ_DATA_PROJECT_ID=BQ_DATA_PROJECT_ID
        )
    else:
        raise ValueError(f"Unsupported generate_sql_type: {generate_sql_type}")

    model = GeminiModel(model_name=model, temperature=temperature)
    requests = [prompt for _ in range(number_of_candidates)]
    responses = model.call_parallel(requests, parser_func=parse_response)
    # Take just the first response.
    responses = responses[0]

    # If postprocessing of the SQL to transpile it to BigQuery is required,
    # then do it here.
    if transpile_to_bigquery:
        translator = sql_translator.SqlTranslator(
            model=model,
            temperature=temperature,
            process_input_errors=process_input_errors,
            process_tool_output_errors=process_tool_output_errors,
        )
        # pylint: disable=g-bad-todo
        # pylint: enable=g-bad-todo
        responses: str = translator.translate(
            responses, ddl_schema=ddl_schema, db=db, catalog=project
        )

    return responses



================================================
FILE: adk-references/agents-tw/data-science/data_science/sub_agents/bigquery/chase_sql/dc_prompt_template.py
================================================
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Divide-and-Conquer prompt template."""

DC_PROMPT_TEMPLATE = """
You are an experienced database expert.
Now you need to generate a GoogleSQL or BigQuery query given the database information, a question and some additional information.
The database structure is defined by table schemas (some columns provide additional column descriptions in the options).

Given the table schema information description and the `Question`. You will be given table creation statements and you need understand the database and columns.

You will be using a way called "recursive divide-and-conquer approach to SQL query generation from natural language".

Here is a high level description of the steps.
1. **Divide (Decompose Sub-question with Pseudo SQL):** The complex natural language question is recursively broken down into simpler sub-questions. Each sub-question targets a specific piece of information or logic required for the final SQL query.
2. **Conquer (Real SQL for sub-questions):**  For each sub-question (and the main question initially), a "pseudo-SQL" fragment is formulated. This pseudo-SQL represents the intended SQL logic but might have placeholders for answers to the decomposed sub-questions.
3. **Combine (Reassemble):** Once all sub-questions are resolved and their corresponding SQL fragments are generated, the process reverses. The SQL fragments are recursively combined by replacing the placeholders in the pseudo-SQL with the actual generated SQL from the lower levels.
4. **Final Output:** This bottom-up assembly culminates in the complete and correct SQL query that answers the original complex question.

Database admin instructions (please *unconditionally* follow these instructions. Do *not* ignore them or use them as hints.):
1. **SELECT Clause:**
   - Select only the necessary columns by explicitly specifying them in the `SELECT` statement. Avoid redundant columns or values.

2. **Aggregation (MAX/MIN):**
   - Ensure `JOIN`s are completed before applying `MAX()` or `MIN()`. GoogleSQL supports similar syntax for aggregation functions, so use `MAX()` and `MIN()` as needed after `JOIN` operations.

3. **ORDER BY with Distinct Values:**
   - In GoogleSQL, `GROUP BY <column>` can be used before `ORDER BY <column> ASC|DESC` to get distinct values and sort them.

4. **Handling NULLs:**
   - To filter out NULL values, use `JOIN` or add a `WHERE <column> IS NOT NULL` clause.

5. **FROM/JOIN Clauses:**
   - Only include tables essential to the query. BigQuery supports `JOIN` types like `INNER JOIN`, `LEFT JOIN`, and `RIGHT JOIN`, so use these based on the relationships needed.

6. **Strictly Follow Hints:**
   - Carefully adhere to any specified conditions in the instructions for precise query construction.

7. **Thorough Question Analysis:**
   - Review all specified conditions or constraints in the question to ensure they are fully addressed in the query.

8. **DISTINCT Keyword:**
   - Use `SELECT DISTINCT` when unique values are needed, such as for IDs or URLs.

9. **Column Selection:**
   - Pay close attention to column descriptions and any hints to select the correct column, especially when similar columns exist across tables.

10. **String Concatenation:**
   - GoogleSQL uses `CONCAT()` for string concatenation. Avoid using `||` and instead use `CONCAT(column1, ' ', column2)` for concatenation.

11. **JOIN Preference:**
   - Use `INNER JOIN` when appropriate, and avoid nested `SELECT` statements if a `JOIN` will achieve the same result.

12. **GoogleSQL Functions Only:**
   - Use functions available in GoogleSQL. Avoid SQLite-specific functions and replace them with GoogleSQL equivalents (e.g., `FORMAT_DATE` instead of `STRFTIME`).

13. **Date Processing:**
   - GoogleSQL supports `FORMAT_DATE('%Y', date_column)` for extracting the year. Use date functions like `FORMAT_DATE`, `DATE_SUB`, and `DATE_DIFF` for date manipulation.

14. **Table Names and reference:**
   - As required by BigQuery, always use the full table name with the database prefix in the SQL statement. For example, "SELECT * FROM example_bigquery_database.table_a", not just "SELECT * FROM table_a"

15. **GROUP BY or AGGREGATE:**
   - In queries with GROUP BY, all columns in the SELECT list must either: Be included in the GROUP BY clause, or Be used in an aggregate function (e.g., MAX, MIN, AVG, COUNT, SUM).

Here are some examples
===========
Example 1

**************************
【Table creation statements】
CREATE TABLE `{BQ_DATA_PROJECT_ID}`.restaurant.generalinfo
(
 id_restaurant INT64,
 food_type STRING OPTIONS(description="the food type"),
 city STRING OPTIONS(description="the city where the restaurant is located in"),
);

CREATE TABLE `{BQ_DATA_PROJECT_ID}`.restaurant.location
(
 id_restaurant INT64,
 street_name STRING OPTIONS(description="the street name of the restaurant"),
 city STRING OPTIONS(description="the city where the restaurant is located in foreign key (id_restaurant) references generalinfo (id_restaurant) on update cascade on delete cascade"),
);

**************************
【Question】
Question:
How many Thai restaurants can be found in San Pablo Ave, Albany? Thai restaurant refers to food_type = 'thai'; San Pablo Ave Albany refers to street_name = 'san pablo ave' AND T1.city = 'albany'

**************************
【Answer】
Repeating the question and generating the SQL with Recursive Divide-and-Conquer.
**Question**: How many Thai restaurants can be found in San Pablo Ave, Albany? Thai restaurant refers to food_type = 'thai'; San Pablo Ave Albany refers to street_name = 'san pablo ave' AND T1.city = 'albany'

**1. Divide and Conquer:**

* **Main Question:** How many Thai restaurants can be found in San Pablo Ave, Albany?
   * **Analysis:** The question asks for a count of restaurants, so we'll use `COUNT()` for that. The count should include only Thai restaurants, which we can identify using the `food_type` column in the `restaurant.generalinfo` table.  The location "San Pablo Ave, Albany" spans two columns (`street_name` and `city`) in the `restaurant.location` table, requiring us to join these two tables.
   * **Pseudo SQL:** SELECT COUNT(`T1`.`id_restaurant`) FROM `restaurantgeneralinfo` AS `T1` INNER JOIN `restaurant.location` AS `T2` ON `T1`.`id_restaurant` = `T2`.`id_restaurant` WHERE  <Thai restaurant> AND <in San Pablo Ave, Albany>

   * **Sub-question 1:** Thai restaurant
       * **Analysis:** This is a straightforward filter on the `restaurant.generalinfo` table using the `food_type` column.
       * **Pseudo SQL:** `T1`.`food_type` = 'thai'

   * **Sub-question 2:** in San Pablo Ave, Albany
       * **Analysis:** This location information is spread across two columns in the `restaurant.location` table. We need to combine these conditions with an "AND" operator to ensure both are met.
       * **Pseudo SQL:** `T2`.`street_name` = 'san pablo ave' AND `T2`.`city` = 'albany'

**2. Assembling SQL:**

* **Sub-question 1 (Thai restaurant):**
   * **SQL:** `T1`.`food_type` = 'thai'

* **Sub-question 2 (in San Pablo Ave, Albany):**
   * **SQL:** `T2`.`street_name` = 'san pablo ave' AND `T2`.`city` = 'albany'

* **Main Question (count of restaurants):**
   * **SQL:** SELECT COUNT(`T1`.`id_restaurant`) FROM `{BQ_DATA_PROJECT_ID}.restaurant.generalinfo` AS `T1` INNER JOIN `{BQ_DATA_PROJECT_ID}.restaurant.location` AS `T2` ON `T1`.`id_restaurant` = `T2`.`id_restaurant` WHERE `T1`.`food_type` = 'thai' AND `T2`.`street_name` = 'san pablo ave' AND `T2`.`city` = 'albany'

**3. Simplification and Optimization:**

* The SQL query from step 2 is already quite efficient. We've used `INNER JOIN` to combine the tables based on their relationship, and the `WHERE` clause clearly defines our filtering criteria. There's no need for nested queries or complex sub-selections in this case.

**Final Optimized SQL Query:**
```sql
SELECT COUNT(T1.id_restaurant)
 FROM `{BQ_DATA_PROJECT_ID}`.restaurant.generalinfo AS T1
 INNER JOIN `{BQ_DATA_PROJECT_ID}`.restaurant.location AS T2 ON T1.id_restaurant = T2.id_restaurant
 WHERE T1.food_type = 'thai' AND T1.city = 'albany' AND T2.street_name = 'san pablo ave'
```

===========
Example 2

**************************
【Database Info】
CREATE TABLE `{BQ_DATA_PROJECT_ID}`.financial.account (
   account_id INT64,
   district_id INT64,
   frequency STRING,
   date DATE,
);
CREATE TABLE `{BQ_DATA_PROJECT_ID}`.financial.client (
   client_id INT64,
   gender STRING,
   birth_date DATE,
   district_id INT64,
);
CREATE TABLE `{BQ_DATA_PROJECT_ID}`.financial.district (
   district_id INT64,
   a4 STRING OPTIONS(description="Assuming A4 and A11 are strings due to examples"),
   a11 STRING,
);

**************************
【Question】
Question:
What is the gender of the youngest client who opened account in the lowest average salary branch? Given that Later birthdate refers to younger age; A11 refers to average salary

**************************
【Answer】
Repeating the question and generating the SQL with Recursive Divide-and-Conquer.
**Question**: What is the gender of the youngest client who opened account in the lowest average salary branch? Given that Later birthdate refers to younger age; A11 refers to average salary

**1. Divide and Conquer:**

* **Main Question:** What is the gender of the youngest client who opened account in the lowest average salary branch?
   * **Analysis:** The question is asking about `gender`, and it appears in the table `financial.client`. We will use this as the output column, selecting it from the youngest client in the lowest average salary branch.
   * **Pseudo **Final Optimized SQL Query:**** SELECT `T1`.`gender` FROM `{BQ_DATA_PROJECT_ID}.financial.client` AS `T1` WHERE <youngest client in the lowest average salary branch>

   * **Sub-question 1:** youngest client in the lowest average salary branch
       * **Analysis:** According to the hint, we need to use the `A11` from `financial.district` to get the salary info, and the youngest client can be obtained from using the `birth_date` column of table `financial.client`. The items between these two tables can be INNER JOIN using district_id.
       * **Pseudo SQL:** SELECT `T1`.`client_id` FROM `{BQ_DATA_PROJECT_ID}.financial.client` AS `T1` INNER JOIN `{BQ_DATA_PROJECT_ID}.financial.district` AS `T2` ON `T1`.`district_id` = `T2`.`district_id` WHERE <lowest average salary branch> ORDER BY `T1`.`birth_date` DESC NULLS LAST LIMIT 1

       * **Sub-question 1.1:** lowest average salary branch
           * **Analysis:** We can get the lowest average salary branch using order by `A11` ASC and pick top 1. The column `A11` is not NULLABLE, so we do not need to add "IS NOT NULL" filter
           * **Pseudo SQL:**  SELECT `district_id` FROM `{BQ_DATA_PROJECT_ID}.financial.district` ORDER BY `A11` ASC LIMIT 1

**2. Assembling SQL:**

* **Sub-question 1.1 (lowest average salary branch):**
   * **SQL:** SELECT `district_id` FROM `{BQ_DATA_PROJECT_ID}.financial.district` ORDER BY `A11` ASC LIMIT 1

* **Sub-question 1 (youngest client in the lowest average salary branch):**
   * **SQL:** SELECT `T1`.`client_id` FROM `{BQ_DATA_PROJECT_ID}.financial.client` AS `T1` INNER JOIN `{BQ_DATA_PROJECT_ID}.financial.district` AS `T2` ON `T1`.`district_id` = `T2`.`district_id` WHERE `T2`.`district_id` IN (SELECT `district_id` FROM `financial.district` ORDER BY `A11` ASC LIMIT 1) ORDER BY `T1`.`birth_date` DESC NULLS LAST LIMIT 1

* **Main Question (gender of the client):**
   * **SQL:** SELECT `T1`.`gender` FROM `{BQ_DATA_PROJECT_ID}.financial.client` AS `T1` WHERE `T1`.`client_id` = (SELECT `T1`.`client_id` FROM `{BQ_DATA_PROJECT_ID}.financial.client` AS `T1` INNER JOIN `{BQ_DATA_PROJECT_ID}.financial.district` AS `T2` ON `T1`.`district_id` = `T2`.`district_id` WHERE `T2`.`district_id` IN (SELECT `district_id` FROM `{BQ_DATA_PROJECT_ID}.financial.district` ORDER BY `A11` ASC LIMIT 1) ORDER BY `T1`.`birth_date` DESC NULLS LAST LIMIT 1)

**3. Simplification and Optimization:**

* The final SQL query from step 2 can be simplified and optimized. The nested queries can be combined using a single `INNER JOIN` and the filtering can be done within a single `ORDER BY` clause.

**Final Optimized SQL Query:**
```sql
SELECT `T1`.`gender`
 FROM `{BQ_DATA_PROJECT_ID}.financial.client` AS `T1`
 INNER JOIN `{BQ_DATA_PROJECT_ID}.financial.district` AS `T2`
 ON `T1`.`district_id` = `T2`.`district_id`
 ORDER BY `T2`.`A11` ASC, `T1`.`birth_date` DESC NULLS LAST
 LIMIT 1
```
===========
Example 3 (dividing into two parallel sub-questions)

**************************
【Database Info】
CREATE TABLE `{BQ_DATA_PROJECT_ID}`.olympics.games
(
 id INT64,
 games_year INT64 OPTIONS(description="description: the year of the game"),
 games_name STRING,
);

CREATE TABLE `{BQ_DATA_PROJECT_ID}`.olympics.games_city
(
 games_id INT64,
 city_id INT64 OPTIONS(description="the id of the city that held the game Maps to city(id)"),
);

CREATE TABLE `{BQ_DATA_PROJECT_ID}`.olympics.city
(
 id INT64,
 city_name STRING,
);

**************************
【Question】
Question:
From 1900 to 1992, how many games did London host? From 1900 to 1992 refers to games_year BETWEEN 1900 AND 1992; London refers to city_name = 'London'; games refer to games_name;

**************************
【Answer】
Repeating the question and generating the SQL with Recursive Divide-and-Conquer.
**Question**: From 1900 to 1992, how many games did London host? From 1900 to 1992 refers to games_year BETWEEN 1900 AND 1992; London refers to city_name = 'London'; games refer to games_name;

**1. Divide and Conquer:**

* **Main Question:** From 1900 to 1992, how many games did London host?
   * **Analysis:** The question requires us to count games, which are represented by the `id` column in the `olympics.games` table.  We need to filter these games based on two criteria: they were hosted in London and occurred between 1900 and 1992.
   * **Pseudo SQL:** SELECT COUNT(`T1`.`id`) FROM `{BQ_DATA_PROJECT_ID}.olympics.games` AS `T1`  WHERE  <games are in London> AND <games year between 1900 and 1992>

   * **Sub-question 1:** games are in London
       * **Analysis:**  To determine which games were hosted in London, we need to join the `olympics.games` table with the `olympics.games_city` table on `games_id` and then join with the `city` table on `city_id`. We'll use `INNER JOIN` to ensure only matching records are considered.  The filtering on 'London' will be applied to the `city_name` column.
       * **Pseudo SQL:**  `T1`.`id` IN (SELECT `T1`.`games_id` FROM `{BQ_DATA_PROJECT_ID}.olympics.games_city` AS `T1` INNER JOIN `{BQ_DATA_PROJECT_ID}.olympics.city` AS `T2` ON `T1`.`city_id` = `T2`.`id` WHERE `T2`.`city_name` = 'London')

   * **Sub-question 2:** games year between 1900 and 1992
       * **Analysis:** This involves filtering the `olympics.games` table directly based on the `games_year` column using the `BETWEEN` operator.
       * **Pseudo SQL:** `T1`.`games_year` BETWEEN 1900 AND 1992

**2. Assembling SQL:**

* **Sub-question 1 (games are in London):**
   * **SQL:**  `T1`.`id` IN (SELECT `T1`.`games_id` FROM `{BQ_DATA_PROJECT_ID}.olympics.games_city` AS `T1` INNER JOIN `{BQ_DATA_PROJECT_ID}.olympics.city` AS `T2` ON `T1`.`city_id` = `T2`.`id` WHERE `T2`.`city_name` = 'London')

* **Sub-question 2 (games year between 1900 and 1992):**
   * **SQL:**  `T1`.`games_year` BETWEEN 1900 AND 1992

* **Main Question (count of games):**
   * **SQL:** SELECT COUNT(`T1`.`id`) FROM `{BQ_DATA_PROJECT_ID}.olympics.games` AS `T1` WHERE `T1`.`id` IN (SELECT `T1`.`games_id` FROM `{BQ_DATA_PROJECT_ID}.olympics.games_city` AS `T1` INNER JOIN `{BQ_DATA_PROJECT_ID}.olympics.city` AS `T2` ON `T1`.`city_id` = `T2`.`id` WHERE `T2`.`city_name` = 'London') AND `T1`.`games_year` BETWEEN 1900 AND 1992

**3. Simplification and Optimization:**

* The nested query can be converted into a more efficient `JOIN` operation. We'll use `INNER JOIN` to combine ``{BQ_DATA_PROJECT_ID}`.olympics.games`, ``{BQ_DATA_PROJECT_ID}`.olympics.games_city`, and ``{BQ_DATA_PROJECT_ID}`.olympics.city` based on the relationships between them.

**Final Optimized SQL Query:**
```sql
SELECT COUNT(T3.id)
 FROM `{BQ_DATA_PROJECT_ID}`.olympics.games_city AS T1
 INNER JOIN `{BQ_DATA_PROJECT_ID}`.olympics.city AS T2 ON T1.city_id = T2.id
 INNER JOIN `{BQ_DATA_PROJECT_ID}`.olympics.games AS T3 ON T1.games_id = T3.id
 WHERE T2.city_name = 'London' AND T3.games_year
 BETWEEN 1900 AND 1992
```

===========
Example 4 (When it's not clear which column should be used for a string matching, use a loosen condition such as string LIKE and OR condition to cover multiple possible columns.)

**************************
【Database Info】
CREATE TABLE `{BQ_DATA_PROJECT_ID}.academics.student_programs` (
   `Program Type` STRING,
   `Participants (Ages 10-15)` FLOAT64,
   `Total Enrollment (Ages 10-15)` FLOAT64,
   `School Category` STRING,
);

**************************
【Question】
Question:
Please list the lowest three participation rates for students aged 10-15 in online programs. Participation rate for students aged 10-15 = `Participants (Ages 10-15)` / `Total Enrollment (Ages 10-15)`

**************************
【Answer】
Repeating the question and generating the SQL with Recursive Divide-and-Conquer.
**Question:** Please list the lowest three participation rates for students aged 10-15 in online programs. Participation rate for students aged 10-15 = `Participants (Ages 10-15)` / `Total Enrollment (Ages 10-15)`

**1. Divide and Conquer:**

* **Main Question:** Please list the lowest three participation rates for students aged 10-15 in online programs.
   * **Analysis:** The question is asking about the ratio between `Participants (Ages 10-15)` and `Total Enrollment (Ages 10-15)`. We need to filter the data to only include online programs.
   * **Pseudo SQL:** SELECT (`Participants (Ages 10-15)` / `Total Enrollment (Ages 10-15)`) FROM `{BQ_DATA_PROJECT_ID}.academics.student_programs` WHERE <online programs> ORDER BY (`Participants (Ages 10-15)` / `Total Enrollment (Ages 10-15)`) ASC NULLS LAST LIMIT 3

   * **Sub-question 1:** online programs
       * **Analysis:** We will get the information from the table `{BQ_DATA_PROJECT_ID}.academics.student_programs`.
       * **Pseudo SQL:** SELECT program_id FROM `academics.student_programs` WHERE <condition for online programs>

       * **Sub-question 1.1:** condition for online programs (Note: This requires external knowledge or database schema information. We need to identify which column(s) indicate "online programs".)
           * **Analysis:** We'll assume either "School Category" or "Program Type" columns might contain the term "online."
           * **Pseudo SQL:**  LOWER(`School Category`) LIKE '%online%' OR LOWER(`Program Type`) LIKE '%online%'

**2. Assembling SQL:**

* **Sub-question 1.1 (condition for online programs):**
   * **SQL:** LOWER(`School Category`) LIKE '%online%' OR LOWER(`Program Type`) LIKE '%online%'

* **Sub-question 1 (online programs):**
   * **SQL:** SELECT program_id FROM `{BQ_DATA_PROJECT_ID}.academics.student_programs` WHERE LOWER(`School Category`) LIKE '%online%' OR LOWER(`Program Type`) LIKE '%online%'

* **Main Question (lowest three participation rates):**
   * **SQL:** SELECT (`Participants (Ages 10-15)` / `Total Enrollment (Ages 10-15)`) FROM `{BQ_DATA_PROJECT_ID}.academics.student_programs` WHERE program_id IN (SELECT program_id FROM `{BQ_DATA_PROJECT_ID}.academics.student_programs` WHERE LOWER(`School Category`) LIKE '%online%' OR LOWER(`Program Type`) LIKE '%online%') ORDER BY (`Participants (Ages 10-15)` / `Total Enrollment (Ages 10-15)`) ASC NULLS LAST LIMIT 3

**3. Simplification and Optimization:**

* We can directly incorporate the condition for online programs into the main query.

**Final Optimized SQL Query:**
```sql
SELECT `Participants (Ages 10-15)` / `Total Enrollment (Ages 10-15)` FROM `{BQ_DATA_PROJECT_ID}.academics.student_programs`
 WHERE LOWER(`School Category`) LIKE '%online%' OR LOWER(`Program Type`) LIKE '%online%'
 AND `Participants (Ages 10-15)` / `Total Enrollment (Ages 10-15)` IS NOT NULL
 ORDER BY `Participants (Ages 10-15)` / `Total Enrollment (Ages 10-15)` ASC NULLS LAST LIMIT 3;
```

===========
Example 5

**************************
【Database Info】
CREATE TABLE `{BQ_DATA_PROJECT_ID}`.retails.employees (
   employee_id INT64,
   department_id INT64,
   salary INT64,
);

**************************
【Question】
Question:
How many employees earn over $100,000?

**************************
【Answer】
Repeating the question and generating the SQL with Recursive Divide-and-Conquer.
**Question:** How many employees earn over $100,000?

**1. Divide and Conquer:**

* **Main Question:** How many employees earn over $100,000?

   * **Pseudo SQL:** SELECT COUNT(*) FROM {BQ_DATA_PROJECT_ID}.retails.employees WHERE <employees earning over 100000>
   * **Analysis:** The question is asking about the COUNT of employees. We need to filter the data to only include employees earning over $100,000.

   * **Sub-question 1:** employees earning over 100000
       * **Analysis:** Simple condition on the `salary` column.
       * **Pseudo SQL:** SELECT employee_id FROM {BQ_DATA_PROJECT_ID}.retails.employees WHERE salary > 100000

**2. Assembling SQL:**

* **Sub-question 1 (employees earning over 100000):**
   * **SQL:** SELECT employee_id FROM `{BQ_DATA_PROJECT_ID}`.retails.employees WHERE salary > 100000

* **Main Question (count of employees):**
   * **SQL:** SELECT COUNT(*) FROM `{BQ_DATA_PROJECT_ID}`.retails.employees WHERE employee_id IN (SELECT employee_id FROM `{BQ_DATA_PROJECT_ID}`.retails.employees WHERE salary > 100000)

**3. Simplification and Optimization:**

* We can achieve the same result more efficiently within a single WHERE clause.

**Final Optimized SQL Query:**
```sql
SELECT COUNT(*) FROM `{BQ_DATA_PROJECT_ID}`.retails.employees WHERE salary > 100000;
```

===========
Example 6

**************************
【Database Info】
CREATE TABLE `{BQ_DATA_PROJECT_ID}`.airlines.Airlines
(
 FL_DATE STRING OPTIONS(description="flight date"),
 ORIGIN STRING OPTIONS(description="airport of origin"),
 DEST STRING OPTIONS(description="Destination airport"),
);

CREATE TABLE `{BQ_DATA_PROJECT_ID}`.airlines.Airports
(
 Code STRING,
 Description STRING,
);

**************************
【Question】
Question:
How many flights were there from San Diego International airport to Los Angeles International airport in the August of 2018?
flights from refers to ORIGIN; San Diego International airport refers to Description = 'San Diego, CA: San Diego International'; flights to refers to DEST; Los Angeles International airport refers to Description = 'Los Angeles, CA: Los Angeles International'; in the August of 2018 refers to FL_DATE like '2018/8%';

**************************
【Answer】
**Repeating the question and generating the SQL with Recursive Divide-and-Conquer.**

**Question**: How many flights were there from San Diego International airport to Los Angeles International airport in the August of 2018? flights from refers to ORIGIN; San Diego International airport refers to Description = 'San Diego, CA: San Diego International'; flights to refers to DEST; Los Angeles International airport refers to Description = 'Los Angeles, CA: Los Angeles International'; in the August of 2018 refers to FL_DATE like '2018/8%';

**1. Divide and Conquer:**

* **Main Question:** How many flights were there from San Diego International airport to Los Angeles International airport in the August of 2018?
   * **Analysis:** The question asks for a count of flights, which can be obtained by counting the `FL_DATE` entries in the `airlines.Airlines` table. We need to apply three filters: flights originating from San Diego International, flights destined for Los Angeles International, and flights occurring in August 2018.
   * **Pseudo SQL:** SELECT COUNT(`FL_DATE`) FROM `{BQ_DATA_PROJECT_ID}.airlines.Airlines` WHERE <flights are in August 2018> AND <flights are from San Diego International> AND <flights are to Los Angeles International>

   * **Sub-question 1:** flights are in August 2018
       * **Analysis:** This filter can be directly applied to the `{BQ_DATA_PROJECT_ID}.airlines.Airlines` table using the `FL_DATE` column and the `LIKE` operator, as indicated by the evidence.
       * **Pseudo SQL:** `FL_DATE` LIKE '2018/8%'

   * **Sub-question 2:** flights are from San Diego International
       * **Analysis:**  We need to find the airport code (`ORIGIN`) corresponding to 'San Diego, CA: San Diego International' from the `{BQ_DATA_PROJECT_ID}.airlines.Airports` table and use it to filter the `airlines.Airlines` table. This requires joining `airlines.Airports` and `airlines.Airlines` based on `airlines.Airports`.`Code` = `airlines.Airlines`.`ORIGIN`.
       * **Pseudo SQL:** `ORIGIN` = (SELECT `T2`.`ORIGIN` FROM `{BQ_DATA_PROJECT_ID}.airlines.Airports` AS `T1` INNER JOIN `{BQ_DATA_PROJECT_ID}.airlines.Airlines` AS `T2` ON `T1`.`Code` = `T2`.`ORIGIN` WHERE `T1`.`Description` = 'San Diego, CA: San Diego International')

   * **Sub-question 3:** flights are to Los Angeles International
       * **Analysis:** Similar to sub-question 2, we need to find the airport code (`DEST`) for 'Los Angeles, CA: Los Angeles International' from the `airlines.Airports` table and use it to filter the `airlines.Airlines` table. This also requires joining `airlines.Airports` and `airlines.Airlines`, but this time on `airlines.Airports`.`Code` = `airlines.Airlines`.`DEST`.
       * **Pseudo SQL:** `DEST` = (SELECT `T4`.`DEST` FROM `{BQ_DATA_PROJECT_ID}.airlines.Airports` AS `T3` INNER JOIN `{BQ_DATA_PROJECT_ID}.airlines.Airlines` AS `T4` ON `T3`.`Code` = `T4`.`DEST` WHERE `T3`.`Description` = 'Los Angeles, CA: Los Angeles International')

**2. Assembling SQL:**

* **Sub-question 1 (flights are in August 2018):**
   * **SQL:** `FL_DATE` LIKE '2018/8%'

* **Sub-question 2 (flights are from San Diego International):**
   * **SQL:** `ORIGIN` = (SELECT DISTINCT `T2`.`ORIGIN` FROM `{BQ_DATA_PROJECT_ID}.airlines.Airports` AS `T1` INNER JOIN `{BQ_DATA_PROJECT_ID}.airlines.Airlines` AS `T2` ON `T1`.`Code` = `T2`.`ORIGIN` WHERE `T1`.`Description` = 'San Diego, CA: San Diego International')

* **Sub-question 3 (flights are to Los Angeles International):**
   * **SQL:** `DEST` = (SELECT DISTINCT `T4`.`DEST` FROM `{BQ_DATA_PROJECT_ID}.airlines.Airports` AS `T3` INNER JOIN `{BQ_DATA_PROJECT_ID}.airlines.Airlines` AS `T4` ON `T3`.`Code` = `T4`.`DEST` WHERE `T3`.`Description` = 'Los Angeles, CA: Los Angeles International')

* **Main Question (count of flights):**
   * **SQL:** SELECT COUNT(`FL_DATE`) FROM `{BQ_DATA_PROJECT_ID}.airlines.Airlines` WHERE `FL_DATE` LIKE '2018/8%' AND `ORIGIN` = (SELECT `T2`.`ORIGIN` FROM `{BQ_DATA_PROJECT_ID}.airlines.Airports` AS `T1` INNER JOIN `{BQ_DATA_PROJECT_ID}.airlines.Airlines` AS `T2` ON `T1`.`Code` = `T2`.`ORIGIN` WHERE `T1`.`Description` = 'San Diego, CA: San Diego International') AND `DEST` = (SELECT `T4`.`DEST` FROM `{BQ_DATA_PROJECT_ID}.airlines.Airports` AS `T3` INNER JOIN `{BQ_DATA_PROJECT_ID}.airlines.Airlines` AS `T4` ON `T3`.`Code` = `T4`.`DEST` WHERE `T3`.`Description` = 'Los Angeles, CA: Los Angeles International')

**3. Simplification and Optimization:**

* The query in step 2 is already quite optimized. We are using nested queries to avoid joining the `airlines.Airports` table multiple times in the main query, which could potentially impact performance.

**Final Optimized SQL Query:**
```sql
SELECT COUNT(FL_DATE)
 FROM `{BQ_DATA_PROJECT_ID}`.airlines.Airlines
 WHERE FL_DATE LIKE '2018/8%'
 AND ORIGIN = (
   SELECT DISTINCT T2.ORIGIN
   FROM `{BQ_DATA_PROJECT_ID}`.airlines.Airports AS T1
   INNER JOIN `{BQ_DATA_PROJECT_ID}`.airlines.Airlines AS T2 ON T1.Code = T2.ORIGIN
   WHERE T1.Description = 'San Diego, CA: San Diego International' )
 AND DEST = (
   SELECT DISTINCT T4.DEST
   FROM `{BQ_DATA_PROJECT_ID}`.airlines.Airports AS T3
   INNER JOIN `{BQ_DATA_PROJECT_ID}`.airlines.Airlines AS T4 ON T3.Code = T4.DEST
   WHERE T3.Description = 'Los Angeles, CA: Los Angeles International' )
```

===========
Example 7

**************************
【Database Info】
CREATE TABLE `{BQ_DATA_PROJECT_ID}`.food_inspection.businesses
(
       `business_id` INT64,
       `name` STRING OPTIONS(description="the name of the eatery"),
);

CREATE TABLE `{BQ_DATA_PROJECT_ID}`.food_inspection.inspections
(
       `business_id` INT64 OPTIONS(description="the unique id of the business"),
       `score` INT64 OPTIONS(description="description: the inspection score"),
       `date` DATE,
);

CREATE TABLE `{BQ_DATA_PROJECT_ID}`.food_inspection.violations
(
       `business_id` INT64,
       `date` DATE,
);

**************************
【Question】
Question:
What are the names of the establishments that met all the required standards for 4 consecutive years?
establishment has the same meaning as business; score of 90 or more refers to score ≥ 90; year(date) = 2015; ; met all required standards for 4 consecutive years refers to COUNT(year(date)) = 4 where score = 100;

**************************
【Answer】
Repeating the question and generating the SQL with Recursive Divide-and-Conquer.

**Question**: What are the names of the establishments that met all the required standards for 4 consecutive years? establishment has the same meaning as business; score of 90 or more refers to score ≥ 90; year(date) = 2015; ; met all required standards for 4 consecutive years refers to COUNT(year(date)) = 4 where score = 100;

**1. Divide and Conquer:**

* **Main Question:** What are the names of the establishments that met all the required standards for 4 consecutive years?
   * **Analysis:** We need to find the names of businesses that have a score of 100 for 4 consecutive years. The `food_inspection.businesses` table contains the `name` and the `{BQ_DATA_PROJECT_ID}.food_inspection.inspections` table contains the `score` and `date`. We will need to join these tables and filter by score. To check for consecutive years, we'll need to group by business and year, then check if each group has a count of 4.
   * **Pseudo SQL:** SELECT DISTINCT `T2`.`name` FROM `{BQ_DATA_PROJECT_ID}.food_inspection.inspections` AS `T1` INNER JOIN `{BQ_DATA_PROJECT_ID}.food_inspection.businesses` AS `T2` ON `T1`.`business_id` = `T2`.`business_id` WHERE  <score = 100> AND <4 consecutive years>

   * **Sub-question 1:** score = 100
       * **Analysis:** This is a simple filter on the `{BQ_DATA_PROJECT_ID}.food_inspection.inspections` table where we select rows with a `score` of 100.
       * **Pseudo SQL:** `T1`.`score` = 100

   * **Sub-question 2:** 4 consecutive years
       * **Analysis:** This is more complex. We need to group the inspections by business and year, then check if the count for each group is 4. To get the year from the `date` column, we'll use the `FORMAT_DATE('%Y', date)` function. We'll also need to use window functions to assign a rank to each year within a business, allowing us to check for consecutiveness.
       * **Pseudo SQL:** `T2`.`name` IN (SELECT `T4`.`name` FROM (SELECT `T3`.`name`, `T3`.`years`, row_number() OVER (PARTITION BY `T3`.`name` ORDER BY `T3`.`years`) AS `rowNumber` FROM (SELECT DISTINCT `name`, FORMAT_DATE('%Y', date) AS `years` FROM `{BQ_DATA_PROJECT_ID}.food_inspection.inspections` AS `T1` INNER JOIN `{BQ_DATA_PROJECT_ID}.food_inspection.businesses` AS `T2` ON `T1`.`business_id` = `T2`.`business_id` WHERE `T1`.`score` = 100) AS `T3`) AS `T4` GROUP BY `T4`.`name`, date(`T4`.`years` || '-01-01', '-' || (`T4`.`rowNumber` - 1) || ' years') HAVING COUNT(`T4`.`years`) = 4)

       * **Sub-question 2.1:** Get distinct businesses and their inspection years where the score is 100
           * **Analysis:** We need to join `{BQ_DATA_PROJECT_ID}.food_inspection.inspections` and `{BQ_DATA_PROJECT_ID}.food_inspection.businesses` tables, filter by `score` = 100, and select distinct business names and their inspection years.
           * **Pseudo SQL:** SELECT DISTINCT `name`, FORMAT_DATE('%Y', date) AS `years` FROM `{BQ_DATA_PROJECT_ID}.food_inspection.inspections` AS `T1` INNER JOIN `{BQ_DATA_PROJECT_ID}.food_inspection.businesses` AS `T2` ON `T1`.`business_id` = `T2`.`business_id` WHERE `T1`.`score` = 100

       * **Sub-question 2.2:** Assign a rank to each year within a business
           * **Analysis:** We'll use the `row_number()` window function to assign a rank to each year within each business, ordered chronologically. This will help us identify consecutive years later.
           * **Pseudo SQL:** SELECT `T3`.`name`, `T3`.`years`, row_number() OVER (PARTITION BY `T3`.`name` ORDER BY `T3`.`years`) AS `rowNumber` FROM `{BQ_DATA_PROJECT_ID}.food_inspection.inspections` AS `T1` INNER JOIN `{BQ_DATA_PROJECT_ID}.food_inspection.businesses` AS `T2` ON `T1`.`business_id` = `T2`.`business_id` WHERE `T1`.`score` = 100` AS `T3`

       * **Sub-question 2.3:** Group by business and consecutive year groups and check if the count is 4
           * **Analysis:** We'll group the results by business name and a calculated date representing the start of each potential 4-year period. This date is calculated by adding (`rowNumber` - 1) years to the first day of the year extracted from the `years` column. We then filter for groups with a count of 4, indicating 4 consecutive years.
           * **Pseudo SQL:** SELECT `T4`.`name` FROM (<previous sub-query>) AS `T4` GROUP BY `T4`.`name`, date(`T4`.`years` || '-01-01', '-' || (`T4`.`rowNumber` - 1) || ' years') HAVING COUNT(`T4`.`years`) = 4

**2. Assembling SQL:**

* **Sub-question 2.1 (distinct businesses and years with score 100):**
   * **SQL:** SELECT DISTINCT `name`, FORMAT_DATE('%Y', date) AS `years` FROM `{BQ_DATA_PROJECT_ID}.food_inspection.inspections` AS `T1` INNER JOIN `{BQ_DATA_PROJECT_ID}.food_inspection.businesses` AS `T2` ON `T1`.`business_id` = `T2`.`business_id` WHERE `T1`.`score` = 100

* **Sub-question 2.2 (assign rank to each year within a business):**
   * **SQL:** SELECT `T3`.`name`, `T3`.`years`, row_number() OVER (PARTITION BY `T3`.`name` ORDER BY `T3`.`years`) AS `rowNumber` FROM (SELECT DISTINCT `name`, FORMAT_DATE('%Y', date) AS `years` FROM `{BQ_DATA_PROJECT_ID}.food_inspection.inspections` AS `T1` INNER JOIN `{BQ_DATA_PROJECT_ID}.food_inspection.businesses` AS `T2` ON `T1`.`business_id` = `T2`.`business_id` WHERE `T1`.`score` = 100) AS `T3`

* **Sub-question 2.3 (group by business and consecutive year groups):**
   * **SQL:** SELECT `T4`.`name` FROM (SELECT `T3`.`name`, `T3`.`years`, row_number() OVER (PARTITION BY `T3`.`name` ORDER BY `T3`.`years`) AS `rowNumber` FROM (SELECT DISTINCT `name`, FORMAT_DATE('%Y', date) AS `years` FROM `{BQ_DATA_PROJECT_ID}.food_inspection.inspections` AS `T1` INNER JOIN `{BQ_DATA_PROJECT_ID}.food_inspection.businesses` AS `T2` ON `T1`.`business_id` = `T2`.`business_id` WHERE `T1`.`score` = 100) AS `T3`) AS `T4` GROUP BY `T4`.`name`, DATE_SUB(DATE(CONCAT(T4.years, '-01-01')), INTERVAL (T4.rowNumber - 1) YEAR)  HAVING COUNT(`T4`.`years`) = 4

* **Sub-question 2 (4 consecutive years):**
   * **SQL:** `T2`.`name` IN (SELECT `T4`.`name` FROM (SELECT `T3`.`name`, `T3`.`years`, row_number() OVER (PARTITION BY `T3`.`name` ORDER BY `T3`.`years`) AS `rowNumber` FROM (SELECT DISTINCT `name`, FORMAT_DATE('%Y', date) AS `years` FROM `{BQ_DATA_PROJECT_ID}.food_inspection.inspections` AS `T1` INNER JOIN `{BQ_DATA_PROJECT_ID}.food_inspection.businesses` AS `T2` ON `T1`.`business_id` = `T2`.`business_id` WHERE `T1`.`score` = 100) AS `T3`) AS `T4` GROUP BY `T4`.`name`, DATE_SUB(DATE(CONCAT(T4.years, '-01-01')), INTERVAL (T4.rowNumber - 1) YEAR)  HAVING COUNT(`T4`.`years`) = 4)

* **Main Question (names of establishments):**
   * **SQL:** SELECT DISTINCT `T2`.`name` FROM `{BQ_DATA_PROJECT_ID}.inspections` AS `T1` INNER JOIN `{BQ_DATA_PROJECT_ID}.businesses` AS `T2` ON `T1`.`business_id` = `T2`.`business_id` WHERE  `T1`.`score` = 100 AND `T2`.`name` IN (SELECT `T4`.`name` FROM (SELECT `T3`.`name`, `T3`.`years`, row_number() OVER (PARTITION BY `T3`.`name` ORDER BY `T3`.`years`) AS `rowNumber` FROM (SELECT DISTINCT `name`, FORMAT_DATE('%Y', date) AS `years` FROM `{BQ_DATA_PROJECT_ID}.food_inspection.inspections` AS `T1` INNER JOIN `{BQ_DATA_PROJECT_ID}.food_inspection.businesses` AS `T2` ON `T1`.`business_id` = `T2`.`business_id` WHERE `T1`.`score` = 100) AS `T3`) AS `T4` GROUP BY `T4`.`name`, DATE_SUB(DATE(CONCAT(T4.years, '-01-01')), INTERVAL (T4.rowNumber - 1) YEAR) HAVING COUNT(`T4`.`years`) = 4)

**3. Simplification and Optimization:**

* The final SQL query from step 2 can be simplified by merging the nested queries into a single query with a `WITH` clause. This improves readability and potentially performance.

**Final Optimized SQL Query:**
```sql
SELECT DISTINCT T4.name
 FROM ( SELECT T3.name, T3.years, row_number()
 OVER (PARTITION BY T3.name ORDER BY T3.years)
 AS rowNumber FROM ( SELECT DISTINCT name, FORMAT_DATE('%Y', date)
 AS years FROM `{BQ_DATA_PROJECT_ID}`.food_inspection.inspections AS T1
 INNER JOIN `{BQ_DATA_PROJECT_ID}`.food_inspection.businesses AS T2 ON T1.business_id = T2.business_id
 WHERE T1.score = 100 ) AS T3 ) AS T4
 GROUP BY T4.name, DATE_SUB(DATE(CONCAT(T4.years, '-01-01')), INTERVAL (T4.rowNumber - 1) YEAR) HAVING COUNT(T4.years) = 4
```
===========
Example 8

**************************
【Database Info】
CREATE TABLE `bigquery-public-data.covid19_symptom_search.symptom_search_sub_region_2_daily`
(
  country_region_code STRING,
  country_region STRING,
  sub_region_1 STRING,
  sub_region_1_code STRING,
  sub_region_2 STRING,
  sub_region_2_code STRING,
  place_id STRING,
  date DATE,
  symptom_Abdominal_obesity FLOAT64,
  symptom_Abdominal_pain FLOAT64,
  symptom_Acne FLOAT64
)
PARTITION BY date
CLUSTER BY country_region_code, sub_region_1_code, sub_region_2_code, sub_region_2;

**************************
【Question】
Question:
Find the day in which the symptom that occurs most frequently is headache.

**************************
【Answer】
Repeating the question and generating the SQL with Recursive Divide-and-Conquer.

**Question**: Find the day in which the symptom that occurs most frequently is headache.

Analysis: We need to determine the day (day of the week) when the frequency of searches for the symptom "headache" is the highest. This involves:
   - Grouping the data by the day of the week.
   - Counting the occurrences of searches for "headache."
   - Sorting the counts in descending order and selecting the day with the highest count.

Pseudo SQL:
   SELECT FORMAT_DATE('%A', date) AS day, COUNT(*) AS headache_count
   FROM `bigquery-public-data.covid19_symptom_search.symptom_search_sub_region_2_daily`
   WHERE symptom_Headache > 0
   GROUP BY day
   ORDER BY headache_count DESC
   LIMIT 1

Sub-question 1: Extract the day of the week from the date column.
   - Analysis: Use the FORMAT_DATE function with the %A format specifier to extract the day name (e.g., "Monday," "Tuesday") from the date column.

Pseudo SQL:
   SELECT FORMAT_DATE('%A', date) AS day
   FROM `bigquery-public-data.covid19_symptom_search.symptom_search_sub_region_2_daily`

Sub-question 2: Filter rows where "headache" searches occurred.
   - Analysis: Only include rows where the symptom "headache" has a positive value (symptom_Headache > 0).

Pseudo SQL:
   SELECT date
   FROM `bigquery-public-data.covid19_symptom_search.symptom_search_sub_region_2_daily`
   WHERE symptom_Headache > 0

Sub-question 3: Count the occurrences of "headache" searches grouped by day of the week.
   - Analysis: After filtering the data for rows where symptom_Headache > 0, group the data by the day of the week and count the number of rows for each day.

Pseudo SQL:
   SELECT FORMAT_DATE('%A', date) AS day, COUNT(*) AS headache_count
   FROM `bigquery-public-data.covid19_symptom_search.symptom_search_sub_region_2_daily`
   WHERE symptom_Headache > 0
   GROUP BY day

Sub-question 4: Sort the results by the count in descending order and get the top day.
   - Analysis: Use the ORDER BY clause to sort by the count of "headache" searches in descending order. Limit the result to 1 to get the top day.

Pseudo SQL:
   SELECT FORMAT_DATE('%A', date) AS day, COUNT(*) AS headache_count
   FROM `bigquery-public-data.covid19_symptom_search.symptom_search_sub_region_2_daily`
   WHERE symptom_Headache > 0
   GROUP BY day
   ORDER BY headache_count DESC
   LIMIT 1

Assembling SQL
   - Combining all sub-questions into the final query:

**Final Optimized SQL Query:**
```sql
SELECT
  FORMAT_DATE('%A', PARSE_DATE('%Y-%m-%d', date)) AS day,
  COUNT(*) AS headache_count
FROM
  `bigquery-public-data`.`covid19_symptom_search`.`symptom_search_country_daily`
GROUP BY
  day
ORDER BY
  headache_count DESC
LIMIT 1;
```

Now is the real question, following the instruction and examples, generate the GoogleSQL with Recursive Divide-and-Conquer approach.
Follow all steps from the strategy. When you get to the final query, output the query string ONLY in the format ```sql ... ```. Make sure you only output one single query.
Table names always should be exactly the same as the table names mentioned in the database schema, for example, `{BQ_DATA_PROJECT_ID}.airlines.Airlines` instead of `Airlines`.

**************************
【Table creation statements】
{SCHEMA}

**************************
【Question】
Question:
{QUESTION}

**************************
【Answer】
Repeating the question and generating the SQL with Recursive Divide-and-Conquer.
"""



================================================
FILE: adk-references/agents-tw/data-science/data_science/sub_agents/bigquery/chase_sql/llm_utils.py
================================================
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""This code contains the LLM utils for the CHASE-SQL Agent."""

import functools
import os
import random
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Callable, List, Optional

import dotenv
import vertexai
from google.cloud import aiplatform
from vertexai.generative_models import (GenerationConfig, HarmBlockThreshold,
                                        HarmCategory)
from vertexai.preview import caching
from vertexai.preview.generative_models import GenerativeModel

dotenv.load_dotenv(override=True)

SAFETY_FILTER_CONFIG = {
    HarmCategory.HARM_CATEGORY_UNSPECIFIED: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
}

GCP_PROJECT = os.getenv("GOOGLE_CLOUD_PROJECT")
GCP_LOCATION = os.getenv("GOOGLE_CLOUD_LOCATION")

GEMINI_AVAILABLE_REGIONS = [
    "europe-west3",
    "australia-southeast1",
    "us-east4",
    "northamerica-northeast1",
    "europe-central2",
    "us-central1",
    "europe-north1",
    "europe-west8",
    "us-south1",
    "us-east1",
    "asia-east2",
    "us-west1",
    "europe-west9",
    "europe-west2",
    "europe-west6",
    "europe-southwest1",
    "us-west4",
    "asia-northeast1",
    "asia-east1",
    "europe-west1",
    "europe-west4",
    "asia-northeast3",
    "asia-south1",
    "asia-southeast1",
    "southamerica-east1",
]
GEMINI_URL = (
    "projects/{GCP_PROJECT}/locations/{region}/publishers/google/models/{model_name}"
)

aiplatform.init(
    project=GCP_PROJECT,
    location=GCP_LOCATION,
)
vertexai.init(project=GCP_PROJECT, location=GCP_LOCATION)


def retry(max_attempts=8, base_delay=1, backoff_factor=2):
    """Decorator to add retry logic to a function.

    Args:
        max_attempts (int): The maximum number of attempts.
        base_delay (int): The base delay in seconds for the exponential backoff.
        backoff_factor (int): The factor by which to multiply the delay for each
          subsequent attempt.

    Returns:
        Callable: The decorator function.
    """

    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            attempts = 0
            while attempts < max_attempts:
                try:
                    return func(*args, **kwargs)
                except Exception as e:  # pylint: disable=broad-exception-caught
                    print(f"Attempt {attempts + 1} failed with error: {e}")
                    attempts += 1
                    if attempts >= max_attempts:
                        raise e
                    delay = base_delay * (backoff_factor**attempts)
                    delay = delay + random.uniform(0, 0.1 * delay)
                    time.sleep(delay)

        return wrapper

    return decorator


class GeminiModel:
    """Class for the Gemini model."""

    def __init__(
        self,
        model_name: str = "gemini-2.5-flash",
        finetuned_model: bool = False,
        distribute_requests: bool = False,
        cache_name: str | None = None,
        temperature: float = 0.01,
        **kwargs,
    ):
        self.model_name = model_name
        self.finetuned_model = finetuned_model
        self.arguments = kwargs
        self.distribute_requests = distribute_requests
        self.temperature = temperature
        model_name = self.model_name
        if not self.finetuned_model and self.distribute_requests:
            random_region = random.choice(GEMINI_AVAILABLE_REGIONS)
            model_name = GEMINI_URL.format(
                GCP_PROJECT=GCP_PROJECT,
                region=random_region,
                model_name=self.model_name,
            )
        if cache_name is not None:
            cached_content = caching.CachedContent(cached_content_name=cache_name)
            self.model = GenerativeModel.from_cached_content(
                cached_content=cached_content
            )
        else:
            self.model = GenerativeModel(model_name=model_name)

    @retry(max_attempts=12, base_delay=2, backoff_factor=2)
    def call(self, prompt: str, parser_func=None) -> str:
        """Calls the Gemini model with the given prompt.

        Args:
            prompt (str): The prompt to call the model with.
            parser_func (callable, optional): A function that processes the LLM
              output. It takes the model"s response as input and returns the
              processed result.

        Returns:
            str: The processed response from the model.
        """
        response = self.model.generate_content(
            prompt,
            generation_config=GenerationConfig(
                temperature=self.temperature,
                **self.arguments,
            ),
            safety_settings=SAFETY_FILTER_CONFIG,
        ).text
        if parser_func:
            return parser_func(response)
        return response

    def call_parallel(
        self,
        prompts: List[str],
        parser_func: Optional[Callable[[str], str]] = None,
        timeout: int = 60,
        max_retries: int = 5,
    ) -> List[Optional[str]]:
        """Calls the Gemini model for multiple prompts in parallel using threads with retry logic.

        Args:
            prompts (List[str]): A list of prompts to call the model with.
            parser_func (callable, optional): A function to process each response.
            timeout (int): The maximum time (in seconds) to wait for each thread.
            max_retries (int): The maximum number of retries for timed-out threads.

        Returns:
            List[Optional[str]]:
            A list of responses, or None for threads that failed.
        """
        results = [None] * len(prompts)

        def worker(index: int, prompt: str):
            """Thread worker function to call the model and store the result with retries."""
            retries = 0
            while retries <= max_retries:
                try:
                    return self.call(prompt, parser_func)
                except Exception as e:  # pylint: disable=broad-exception-caught
                    print(f"Error for prompt {index}: {str(e)}")
                    retries += 1
                    if retries <= max_retries:
                        print(f"Retrying ({retries}/{max_retries}) for prompt {index}")
                        time.sleep(1)  # Small delay before retrying
                    else:
                        return f"Error after retries: {str(e)}"

        # Create and start one thread for each prompt
        with ThreadPoolExecutor(max_workers=len(prompts)) as executor:
            future_to_index = {
                executor.submit(worker, i, prompt): i
                for i, prompt in enumerate(prompts)
            }

            for future in as_completed(future_to_index, timeout=timeout):
                index = future_to_index[future]
                try:
                    results[index] = future.result()
                except Exception as e:  # pylint: disable=broad-exception-caught
                    print(f"Unhandled error for prompt {index}: {e}")
                    results[index] = "Unhandled Error"

        # Handle remaining unfinished tasks after the timeout
        for future in future_to_index:
            index = future_to_index[future]
            if not future.done():
                print(f"Timeout occurred for prompt {index}")
                results[index] = "Timeout"

        return results



================================================
FILE: adk-references/agents-tw/data-science/data_science/sub_agents/bigquery/chase_sql/qp_prompt_template.py
================================================
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Query Plan (QP) prompt template."""

QP_PROMPT_TEMPLATE = """
You are an experienced database expert.
Now you need to generate a GoogleSQL or BigQuery query given the database information, a question and some additional information.
The database structure is defined by table schemas (some columns provide additional column descriptions in the options).

Given the table schema information description and the `Question`. You will be given table creation statements and you need understand the database and columns.

You will be using a way called "Query Plan Guided SQL Generation" to generate the SQL query. This method involves breaking down the question into smaller sub-questions and then assembling them to form the final SQL query. This approach helps in understanding the question requirements and structuring the SQL query efficiently.

Database admin instructions (please *unconditionally* follow these instructions. Do *not* ignore them or use them as hints.):
1. **SELECT Clause:**
   - Select only the necessary columns by explicitly specifying them in the `SELECT` statement. Avoid redundant columns or values.

2. **Aggregation (MAX/MIN):**
   - Ensure `JOIN`s are completed before applying `MAX()` or `MIN()`. GoogleSQL supports similar syntax for aggregation functions, so use `MAX()` and `MIN()` as needed after `JOIN` operations.

3. **ORDER BY with Distinct Values:**
   - In GoogleSQL, `GROUP BY <column>` can be used before `ORDER BY <column> ASC|DESC` to get distinct values and sort them.

4. **Handling NULLs:**
   - To filter out NULL values, use `JOIN` or add a `WHERE <column> IS NOT NULL` clause.

5. **FROM/JOIN Clauses:**
   - Only include tables essential to the query. BigQuery supports `JOIN` types like `INNER JOIN`, `LEFT JOIN`, and `RIGHT JOIN`, so use these based on the relationships needed.

6. **Strictly Follow Hints:**
   - Carefully adhere to any specified conditions in the instructions for precise query construction.

7. **Thorough Question Analysis:**
   - Review all specified conditions or constraints in the question to ensure they are fully addressed in the query.

8. **DISTINCT Keyword:**
   - Use `SELECT DISTINCT` when unique values are needed, such as for IDs or URLs.

9. **Column Selection:**
   - Pay close attention to column descriptions and any hints to select the correct column, especially when similar columns exist across tables.

10. **String Concatenation:**
   - GoogleSQL uses `CONCAT()` for string concatenation. Avoid using `||` and instead use `CONCAT(column1, ' ', column2)` for concatenation.

11. **JOIN Preference:**
   - Use `INNER JOIN` when appropriate, and avoid nested `SELECT` statements if a `JOIN` will achieve the same result.

12. **GoogleSQL Functions Only:**
   - Use functions available in GoogleSQL. Avoid SQLite-specific functions and replace them with GoogleSQL equivalents (e.g., `FORMAT_DATE` instead of `STRFTIME`).

13. **Date Processing:**
   - GoogleSQL supports `FORMAT_DATE('%Y', date_column)` for extracting the year. Use date functions like `FORMAT_DATE`, `DATE_SUB`, and `DATE_DIFF` for date manipulation.

14. **Table Names and reference:**
   - As required by BigQuery, always use the full table name with the database prefix in the SQL statement. For example, "SELECT * FROM example_bigquery_database.table_a", not just "SELECT * FROM table_a"

15. **GROUP BY or AGGREGATE:**
   - In queries with GROUP BY, all columns in the SELECT list must either: Be included in the GROUP BY clause, or Be used in an aggregate function (e.g., MAX, MIN, AVG, COUNT, SUM).

Here are some examples
===========
Example 1

**************************
【Table creation statements】
CREATE TABLE `{BQ_DATA_PROJECT_ID}`.restaurant.generalinfo
(
 id_restaurant INT64,
 food_type STRING OPTIONS(description="the food type"),
 city STRING OPTIONS(description="the city where the restaurant is located in"),
);

CREATE TABLE `{BQ_DATA_PROJECT_ID}`.restaurant.location
(
 id_restaurant INT64,
 street_name STRING OPTIONS(description="the street name of the restaurant"),
 city STRING OPTIONS(description="the city where the restaurant is located in foreign key (id_restaurant) references generalinfo (id_restaurant) on update cascade on delete cascade"),
);

**************************
【Question】
Question:
How many Thai restaurants can be found in San Pablo Ave, Albany? Thai restaurant refers to food_type = 'thai'; San Pablo Ave Albany refers to street_name = 'san pablo ave' AND T1.city = 'albany'

**************************
【Answer】
Repeating the question and generating the SQL with Recursive Divide-and-Conquer.
**Question**: How many Thai restaurants can be found in San Pablo Ave, Albany? Thai restaurant refers to food_type = 'thai'; San Pablo Ave Albany refers to street_name = 'san pablo ave' AND T1.city = 'albany'


**Query Plan**:

** Preparation Steps:**
1. Initialize the process: Start preparing to execute the query.
2. Prepare storage: Set up storage space (registers) to hold temporary results, initializing them to NULL.
3. Open the location table: Open the location table so we can read from it.
4. Open the generalinfo table: Open the generalinfo table so we can read from it.

** Matching Restaurants:**
1. Start reading the location table: Move to the first row in the location table.
2. Check if the street matches: Look at the street_name column of the current row in location. If it’s not "san pablo ave," skip this row.
3. Identify the matching row: Store the identifier (row ID) of this location entry.
4. Find the corresponding row in generalinfo: Use the row ID from location to directly find the matching row in generalinfo.
5. Check if the food type matches: Look at the food_type column in generalinfo. If it’s not "thai," skip this row.
6. Check if the city matches: Look at the city column in generalinfo. If it’s not "albany," skip this row.

** Counting Restaurants:**
1. Prepare to count this match: If all checks pass, prepare to include this row in the final count.
2. Count this match: Increment the count for each row that meets all the criteria.
3. Move to the next row in location: Go back to the location table and move to the next row, repeating the process until all rows are checked.
4. Finalize the count: Once all rows have been checked, finalize the count of matching rows.
5. Prepare the result: Copy the final count to prepare it for output.

** Delivering the Result:**
1. Output the result: Output the final count, which is the number of restaurants that match all the specified criteria.
2. End the process: Stop the query execution process.
3. Setup phase: Before starting the actual query execution, the system prepares the specific values it will be looking for, like "san pablo ave," "thai," and "albany."

**Final Optimized SQL Query:**
```sql
SELECT COUNT(T1.id_restaurant)
 FROM `{BQ_DATA_PROJECT_ID}`.restaurant.generalinfo AS T1
 INNER JOIN `{BQ_DATA_PROJECT_ID}`.restaurant.location AS T2 ON T1.id_restaurant = T2.id_restaurant
 WHERE T1.food_type = 'thai' AND T1.city = 'albany' AND T2.street_name = 'san pablo ave'
```

===========
Example 2

**************************
【Database Info】
CREATE TABLE `{BQ_DATA_PROJECT_ID}`.financial.account (
   account_id INT64,
   district_id INT64,
   frequency STRING,
   date DATE,
);
CREATE TABLE `{BQ_DATA_PROJECT_ID}`.financial.client (
   client_id INT64,
   gender STRING,
   birth_date DATE,
   district_id INT64,
);
CREATE TABLE `{BQ_DATA_PROJECT_ID}`.financial.district (
   district_id INT64,
   a4 STRING OPTIONS(description="Assuming A4 and A11 are strings due to examples"),
   a11 STRING,
);

**************************
【Question】
Question:
What is the gender of the youngest client who opened account in the lowest average salary branch? Given that Later birthdate refers to younger age; A11 refers to average salary

**************************
【Answer】
Repeating the question and generating the SQL with Recursive Divide-and-Conquer.
**Question**: What is the gender of the youngest client who opened account in the lowest average salary branch? Given that Later birthdate refers to younger age; A11 refers to average salary

**Query Plan**:

** Preparation Steps: **
1. Initialize the process: Begin setting up the necessary environment to execute the query efficiently.
2. Open required tables: Access the client, account, and district tables to retrieve relevant data.
3. Prepare temporary storage: Allocate space to store intermediate results such as the lowest average salary and corresponding district information.

** Identify the Branch with Lowest Average Salary: **
1. Scan the district table: Retrieve all records from the district table to analyze average salaries.
2. Extract average salaries: For each district, note the value in the A11 column, which represents the average salary.
3. Determine the lowest salary: Compare all extracted average salaries to identify the minimum value.
4. Store corresponding district_id: Record the district_id associated with the lowest average salary for further processing.

** Find Clients in the Identified District: **
1. Join client and account tables: Merge records where client.client_id matches account.account_id to associate clients with their accounts.
2. Filter by district_id: Select only those records where account.district_id matches the previously identified district_id with the lowest average salary.
3. Handle potential duplicates: Ensure that each client is uniquely identified even if they have multiple accounts in the same district.

** Identify the Youngest Client: **
1. Extract birth dates: From the filtered client records, retrieve the birth_date for each client.
2. Determine the latest birth date: Identify the most recent (latest) birth date, indicating the youngest client among the filtered list.
3. Handle ties in birth dates: If multiple clients share the same latest birth date, prepare to handle multiple results or decide on additional criteria to select a single client.

** Retrieve Gender Information: **
1. Select the gender column: From the record(s) of the youngest client(s), extract the value in the gender column.
2. Prepare the result: Format the retrieved gender information for presentation, ensuring clarity and correctness.

** Finalize and Deliver the Result: **
1. Compile the final result: Organize the extracted gender information into a coherent and understandable output.
2. Clean up resources: Close any open table connections and release temporary storage used during query execution.
3. Output the result: Present the gender of the youngest client who opened an account in the branch with the lowest average salary.

**Final Optimized SQL Query:**
```sql
SELECT `T1`.`gender`
 FROM `{BQ_DATA_PROJECT_ID}.financial.client` AS `T1`
 INNER JOIN `{BQ_DATA_PROJECT_ID}.financial.district` AS `T2`
 ON `T1`.`district_id` = `T2`.`district_id`
 ORDER BY `T2`.`A11` ASC, `T1`.`birth_date` DESC NULLS LAST
 LIMIT 1
```
===========
Example 3 (dividing into two parallel sub-questions)

**************************
【Database Info】
CREATE TABLE `{BQ_DATA_PROJECT_ID}`.olympics.games
(
 id INT64,
 games_year INT64 OPTIONS(description="description: the year of the game"),
 games_name STRING,
);

CREATE TABLE `{BQ_DATA_PROJECT_ID}`.olympics.games_city
(
 games_id INT64,
 city_id INT64 OPTIONS(description="the id of the city that held the game Maps to city(id)"),
);

CREATE TABLE `{BQ_DATA_PROJECT_ID}`.olympics.city
(
 id INT64,
 city_name STRING,
);

**************************
【Question】
Question:
From 1900 to 1992, how many games did London host? From 1900 to 1992 refers to games_year BETWEEN 1900 AND 1992; London refers to city_name = 'London'; games refer to games_name;

**************************
【Answer】
Repeating the question and generating the SQL with Recursive Divide-and-Conquer.
**Question**: From 1900 to 1992, how many games did London host? From 1900 to 1992 refers to games_year BETWEEN 1900 AND 1992; London refers to city_name = 'London'; games refer to games_name;

**Query Plan**:

** Preparation Steps: **
1.Initialize the process: Set up the environment to begin query execution, including necessary variables and temporary storage.
2. Open required tables: Open the games_city, city, and games tables to access relevant data.
3. Prepare filtering values: Set up the specific values to filter the data, such as the year range (1900-1992) and the city name 'London'.

** Filter and Identify Relevant Data: **
1. Scan games_city table: Retrieve records from the games_city table to match games with the cities where they were hosted.
2. Fetch the corresponding city_id: For each row in games_city, extract the city_id to find out which city hosted the game.
3 .Match city_id with city_name: Use the city_id to look up the corresponding city_name in the city table.
4. Filter by city_name = 'London': Select only those rows where the city_name is 'London'.

** Further Filter by Year Range: **
1. Extract games_id: For rows that match 'London', retrieve the games_id from the games_city table.
2. Find matching games_year: Use the games_id to look up the corresponding games_year in the games table.
3. Filter by games_year between 1900 and 1992: Select only those rows where the games_year falls within the specified range (1900-1992).

** Count the Matching Rows: **
1. Initialize the count: Prepare to count the number of matching rows that meet all the criteria.
2. Count the valid entries: For each row that matches the conditions (city_name = 'London' and games_year between 1900 and 1992), increment the count.
3. Store the final count: Once all rows have been processed, store the total count as the final result.

** Finalize and Deliver the Result: **
1. Prepare the result for output: Format the final count of games hosted by London between 1900 and 1992.
2. Output the final count: Deliver the count as the query result.
3. Clean up resources: Close any open table connections and release temporary storage used during query execution.

**Final Optimized SQL Query:**
```sql
SELECT COUNT(T3.id)
 FROM `{BQ_DATA_PROJECT_ID}`.olympics.games_city AS T1
 INNER JOIN `{BQ_DATA_PROJECT_ID}`.olympics.city AS T2 ON T1.city_id = T2.id
 INNER JOIN `{BQ_DATA_PROJECT_ID}`.olympics.games AS T3 ON T1.games_id = T3.id
 WHERE T2.city_name = 'London' AND T3.games_year
 BETWEEN 1900 AND 1992
```

===========
Example 4

**************************
【Database Info】
CREATE TABLE `{BQ_DATA_PROJECT_ID}`.retails.employees (
   employee_id INT64,
   department_id INT64,
   salary INT64,
);

**************************
【Question】
Question:
How many employees earn over $100,000?

**************************
【Answer】
Repeating the question and generating the SQL with Recursive Divide-and-Conquer.
**Question:** How many employees earn over $100,000?

** Query Plan**:

** Preparation Steps: **
1.cInitialize the process: Begin by setting up the environment for query execution, including initializing variables and temporary storage.
2. Open the employees table: Access the employees table to retrieve the relevant data.

** Filtering Employees by Salary: **
1. Scan the employees table: Begin reading rows from the employees table.
2. Fetch the salary column: For each row, retrieve the value from the salary column.
3. Compare salary against $100,000: Check if the salary value is greater than $100,000.
4. Identify matching rows: For rows where the salary exceeds $100,000, prepare to count these entries.

** Counting the Matches: **
1. Initialize the count: Set up a counter to keep track of how many employees meet the salary condition.
2. Increment the count: For each row where the salary is above $100,000, increment the counter.
3. Store the final count: Once all rows have been processed, store the total count of matching employees.

** Finalize and Deliver the Result: **
1. Prepare the result for output: Format the final count for presentation.
2. Output the final count: Deliver the count as the query result, indicating how many employees earn over $100,000.
3. Clean up resources: Close the employees table and release any temporary storage used during query execution.

**Final Optimized SQL Query:**
```sql
SELECT COUNT(*) FROM `{BQ_DATA_PROJECT_ID}`.retails.employees WHERE salary > 100000;
```

===========
Example 6

**************************
【Database Info】
CREATE TABLE `{BQ_DATA_PROJECT_ID}`.airlines.Airlines
(
 FL_DATE STRING OPTIONS(description="flight date"),
 ORIGIN STRING OPTIONS(description="airport of origin"),
 DEST STRING OPTIONS(description="Destination airport"),
);

CREATE TABLE `{BQ_DATA_PROJECT_ID}`.airlines.Airports
(
 Code STRING,
 Description STRING,
);

**************************
【Question】
Question:
How many flights were there from San Diego International airport to Los Angeles International airport in the August of 2018?
flights from refers to ORIGIN; San Diego International airport refers to Description = 'San Diego, CA: San Diego International'; flights to refers to DEST; Los Angeles International airport refers to Description = 'Los Angeles, CA: Los Angeles International'; in the August of 2018 refers to FL_DATE like '2018/8%';

**************************
【Answer】
**Repeating the question and generating the SQL with Recursive Divide-and-Conquer.**

**Question**: How many flights were there from San Diego International airport to Los Angeles International airport in the August of 2018? flights from refers to ORIGIN; San Diego International airport refers to Description = 'San Diego, CA: San Diego International'; flights to refers to DEST; Los Angeles International airport refers to Description = 'Los Angeles, CA: Los Angeles International'; in the August of 2018 refers to FL_DATE like '2018/8%';

** Query Plan**:

** Preparation Steps: **
1. Initialize the process: Set up the environment and prepare for query execution by initializing variables and temporary storage.
2. Open required tables: Open the Airlines and Airports tables to access relevant data.

** Identify Relevant Flights: **
1. Fetch the FL_DATE column: Start reading the FL_DATE column from the Airlines table.
2. Filter by August 2018: Use the condition FL_DATE LIKE '2018/8%' to filter flights that occurred in August 2018.
3. Join with Airports for ORIGIN: Identify flights originating from 'San Diego, CA: San Diego International' by joining the Airlines table with the Airports table on the ORIGIN field.
4. Join with Airports for DEST: Similarly, identify flights destined for 'Los Angeles, CA: Los Angeles International' by joining the Airlines table with the Airports table on the DEST field.

** Count the Matching Flights: **
1. Initialize the count: Set up a counter to keep track of how many flights match the criteria.
2. Increment the count: For each flight that meets the conditions (originating from San Diego International and destined for Los Angeles International in August 2018), increment the counter.
3. Store the final count: Once all rows have been processed, store the total count of matching flights.

** Finalize and Deliver the Result: **
1. Prepare the result for output: Format the final count for presentation, ensuring clarity and correctness.
2. Output the final count: Deliver the count as the query result, indicating how many flights met the specified criteria.
3. Clean up resources: Close any open table connections and release temporary storage used during query execution.

**Final Optimized SQL Query:**
```sql
SELECT COUNT(FL_DATE)
 FROM `{BQ_DATA_PROJECT_ID}`.airlines.Airlines
 WHERE FL_DATE LIKE '2018/8%'
 AND ORIGIN = (
   SELECT DISTINCT T2.ORIGIN
   FROM `{BQ_DATA_PROJECT_ID}`.airlines.Airports AS T1
   INNER JOIN `{BQ_DATA_PROJECT_ID}`.airlines.Airlines AS T2 ON T1.Code = T2.ORIGIN
   WHERE T1.Description = 'San Diego, CA: San Diego International' )
 AND DEST = (
   SELECT DISTINCT T4.DEST
   FROM `{BQ_DATA_PROJECT_ID}`.airlines.Airports AS T3
   INNER JOIN `{BQ_DATA_PROJECT_ID}`.airlines.Airlines AS T4 ON T3.Code = T4.DEST
   WHERE T3.Description = 'Los Angeles, CA: Los Angeles International' )
```

===========
Example 7

**************************
【Database Info】
CREATE TABLE `{BQ_DATA_PROJECT_ID}`.food_inspection.businesses
(
       `business_id` INT64,
       `name` STRING OPTIONS(description="the name of the eatery"),
);

CREATE TABLE `{BQ_DATA_PROJECT_ID}`.food_inspection.inspections
(
       `business_id` INT64 OPTIONS(description="the unique id of the business"),
       `score` INT64 OPTIONS(description="description: the inspection score"),
       `date` DATE,
);

CREATE TABLE `{BQ_DATA_PROJECT_ID}`.food_inspection.violations
(
       `business_id` INT64,
       `date` DATE,
);

**************************
【Question】
Question:
What are the names of the establishments that met all the required standards for 4 consecutive years?
establishment has the same meaning as business; score of 90 or more refers to score ≥ 90; year(date) = 2015; ; met all required standards for 4 consecutive years refers to COUNT(year(date)) = 4 where score = 100;

**************************
【Answer】
Repeating the question and generating the SQL with Recursive Divide-and-Conquer.

**Question**: What are the names of the establishments that met all the required standards for 4 consecutive years? establishment has the same meaning as business; score of 90 or more refers to score ≥ 90; year(date) = 2015; ; met all required standards for 4 consecutive years refers to COUNT(year(date)) = 4 where score = 100;

** Query Plan**:

** Preparation Steps: **
1. Initialize the process: Set up the environment and prepare for query execution, including initializing variables and temporary storage.
2. Open required tables: Open the businesses, inspections, and violations tables to access relevant data.

** Filter and Identify Relevant Inspections: **
1. Scan the inspections table: Start reading rows from the inspections table.
2. Filter by score of 100: Select only those inspections where the score is 100, indicating that the establishment met all required standards.
3. Extract year from the inspection date: Use the FORMAT_DATE('%Y', date) function to extract the year from the inspection date.
4. Join with businesses table: Match each inspection to the corresponding business by joining on business_id.

** Identify Businesses Meeting Standards for 4 Consecutive Years: **
1. Aggregate by business and year: Group the data by business name and the extracted year to count the number of years each business met the required standards.
3. Apply row numbering: Use ROW_NUMBER() with a partition by business name and order by year to identify consecutive years.
3. Filter for 4 consecutive years: Group by business name and ensure that the count of years with the required score is exactly 4, indicating 4 consecutive years of meeting the standards.

** Count and Finalize the Results: **
1. Count the matching businesses: For each business, count the number of years that meet the criteria.
2. Select distinct business names: Extract the names of businesses that have met the required standards for 4 consecutive years.
3. Store and prepare the result: Once all businesses have been processed, store the result and prepare it for output.

** Deliver the Final Result: **
1. Prepare the result for output: Format the final list of business names for presentation.
2. Output the final result: Deliver the names of the businesses that met the required standards for 4 consecutive years.
3. Clean up resources: Close any open table connections and release temporary storage used during query execution.

**Final Optimized SQL Query:**
```sql
SELECT DISTINCT T4.name
 FROM ( SELECT T3.name, T3.years, row_number()
 OVER (PARTITION BY T3.name ORDER BY T3.years)
 AS rowNumber FROM ( SELECT DISTINCT name, FORMAT_DATE('%Y', date)
 AS years FROM `{BQ_DATA_PROJECT_ID}`.food_inspection.inspections AS T1
 INNER JOIN `{BQ_DATA_PROJECT_ID}`.food_inspection.businesses AS T2 ON T1.business_id = T2.business_id
 WHERE T1.score = 100 ) AS T3 ) AS T4
 GROUP BY T4.name, DATE_SUB(DATE(CONCAT(T4.years, '-01-01')), INTERVAL (T4.rowNumber - 1) YEAR) HAVING COUNT(T4.years) = 4
```

===========
Example 8

**************************
【Database Info】
CREATE TABLE `bigquery-public-data.covid19_symptom_search.symptom_search_sub_region_2_daily`
(
  country_region_code STRING,
  country_region STRING,
  sub_region_1 STRING,
  sub_region_1_code STRING,
  sub_region_2 STRING,
  sub_region_2_code STRING,
  place_id STRING,
  date DATE,
  symptom_Abdominal_obesity FLOAT64,
  symptom_Abdominal_pain FLOAT64,
  symptom_Acne FLOAT64
)
PARTITION BY date
CLUSTER BY country_region_code, sub_region_1_code, sub_region_2_code, sub_region_2;

**************************
【Question】
Question:
Find the day in which the symptom that occurs most frequently is headache.

**************************
【Answer】
Repeating the question and generating the SQL with Recursive Divide-and-Conquer.

**Question**: Find the day in which the symptom that occurs most frequently is headache.

** Query Plan**:

**  Preparation Steps : **
1. Initialize the process: Set up the environment and prepare for query execution, including initializing variables and temporary storage.
2. Open the symptom_search_sub_region_2_daily table: Access the table containing daily symptom search data.

** Extract the headache symptom: **
1. Scan the table: Start reading rows from the symptom_search_sub_region_2_daily table.
2. Identify the headache symptom: Look for the column containing the headache symptom data.
3. Extract the headache symptom value: For each row, extract the value from the headache symptom column.
4. Aggregate by date: Group the data by date to count the occurrences of the headache symptom on each day.

** Sort by frequency: **
1. Order the results in descending order of symptom frequency.
2. Limit the results: Extract the single day with the highest count.

** Step 2: Identify Subtasks **
1. Extract relevant symptom column: While "headache" is not explicitly listed, its frequency might be tracked in a related table (e.g., symptom_search_country_daily) as per the given gold query.
2. Group data by day of the week: Use FORMAT_DATE('%A', date) to extract the day of the week from each date.
3. Aggregate by count: Count the occurrences of the "headache" symptom across dates and group by the day of the week.
4. Sort by frequency: Order the results in descending order of symptom frequency.
5. Limit the results: Extract the single day with the highest count.

** Step 3: Formulate the Query **
1. From the subtasks, the query will:
2. Select the day of the week using FORMAT_DATE('%A', date).
3. Aggregate counts grouped by the day.
4. Sort the results by the aggregated count in descending order.
5. Limit the results to the top record.

** Step 4: Construct the Query **
1. Combining all subtasks, the final query is:
2. SELECT COUNT(symptom_headache) FROM `{BQ_DATA_PROJECT_ID}`.covid19_symptom_search.symptom_search_sub_region_2_daily GROUP BY FORMAT_DATE('%A', date) ORDER BY COUNT(symptom_headache) DESC LIMIT 1;

** Step 5: Finalize the Query **
**Final Optimized SQL Query:**
```sql
SELECT
  FORMAT_DATE('%A', PARSE_DATE('%Y-%m-%d', date)) AS day,
  COUNT(*) AS headache_count
FROM
  `{BQ_DATA_PROJECT_ID}`.covid19_symptom_search.symptom_search_country_daily
GROUP BY
  day
ORDER BY
  headache_count DESC
LIMIT 1;
```

Now is the real question, following the instruction and examples, generate the GoogleSQL with Recursive Divide-and-Conquer approach.
Follow all steps from the strategy. When you get to the final query, output the query string ONLY in the format ```sql ... ```. Make sure you only output one single query.

**************************
【Table creation statements】
{SCHEMA}

**************************
【Question】
Question:
{QUESTION}

**************************
【Answer】
Repeating the question and generating the SQL with Recursive Divide-and-Conquer.
"""



================================================
FILE: adk-references/agents-tw/data-science/data_science/sub_agents/bigquery/chase_sql/sql_postprocessor/README.md
================================================
# Support Post-processing of SQL after Agentic Generation.

This tool provides a way to postprocess the SQL generated by the agent.
Currently, it supports the following post-processing steps: 1. Translation of
the SQL from SQLite to BigQuery. 2. Correction of errors in the SQL before and
after translation.

## Usage

Currently, the post-processing is done within the `chase_db_tools.py` agent. To
use this agent, first configure the database agent to use Chase SQL.

Then, to use the postprocessing, set the `transpile_to_bigquery` argument to
`True` in the `chase_nl2sql` function. Optionally, you can also set the
`process_input_errors` and `process_tool_output_errors` arguments to `True` to
have the postprocessor correct errors in the SQL before and after translation.

### Current Defaults:

-   Model: gemini-2.5-flash
-   Temperature: 0.5
-   Number of candidates: 1
-   transpile_to_bigquery: True
-   process_input_errors: False
-   process_tool_output_errors: False



================================================
FILE: adk-references/agents-tw/data-science/data_science/sub_agents/bigquery/chase_sql/sql_postprocessor/__init__.py
================================================
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.




================================================
FILE: adk-references/agents-tw/data-science/data_science/sub_agents/bigquery/chase_sql/sql_postprocessor/correction_prompt_template.py
================================================
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Prompt template for making any corrections to the translation of SQL."""

CORRECTION_PROMPT_TEMPLATE_V1_0 = """
You are an expert in multiple databases and SQL dialects.
You are given a SQL query that is formatted for the SQL dialect:
{sql_dialect}

The SQL query is:
{sql_query}
{schema_insert}
This SQL query could have the following errors:
{errors}

Please correct the SQL query to make sure it is formatted correctly for the SQL dialect:
{sql_dialect}

DO not change any table or column names in the query. However, you may qualify column names with table names.
Do not change any literals in the query.
You may *only* rewrite the query so that it is formatted correctly for the specified SQL dialect.
Do not return any other information other than the corrected SQL query.

Corrected SQL query:
"""



================================================
FILE: adk-references/agents-tw/data-science/data_science/sub_agents/bigquery/chase_sql/sql_postprocessor/sql_translator.py
================================================
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Translator from SQLite to BigQuery."""

import re
from typing import Any, Final

import regex
import sqlglot
import sqlglot.optimizer

from ..llm_utils import GeminiModel  # pylint: disable=g-importing-member
from .correction_prompt_template import (
    CORRECTION_PROMPT_TEMPLATE_V1_0,
)  # pylint: disable=g-importing-member


ColumnSchemaType = tuple[str, str]
AllColumnsSchemaType = list[ColumnSchemaType]
TableSchemaType = tuple[str, AllColumnsSchemaType]
DDLSchemaType = list[TableSchemaType]

SQLGlotColumnsDictType = dict[str, str]
SQLGlotSchemaType = dict[str, Any]

BirdSampleType = dict[str, Any]


def _isinstance_list_of_str_tuples_lists(obj: Any) -> bool:
    """Checks if the object is a list of tuples or listsof strings."""
    return (
        isinstance(obj, list)
        and all([isinstance(v, (tuple, list)) for v in obj])
        and all([isinstance(v[0], str) and isinstance(v[1], str) for v in obj])
    )


def _isinstance_ddl_schema_type(obj: Any) -> bool:
    """Checks if the object is a DDL schema type."""
    # pylint: disable=g-complex-comprehension
    return (
        isinstance(obj, list)
        and all(
            # Every element is a tuple or list.
            [isinstance(v, (tuple, list)) for v in obj]
        )
        and all(
            # First element is a string (table name) and
            # second element is a list (of tuples or lists).
            [isinstance(v[0], str) and isinstance(v[1], list) for v in obj]
        )
        and all(
            # Every element of above list is a tuple or list of strings
            # (column name, column type)
            [_isinstance_list_of_str_tuples_lists(v[1]) for v in obj]
        )
    )
    # pylint: enable=g-complex-comprehension


def _isinstance_sqlglot_schema_type(obj: Any) -> bool:
    """Checks if the object is a SQLGlot schema type."""
    # pylint: disable=g-complex-comprehension
    return (
        isinstance(obj, dict)
        and all([isinstance(v, dict) for v in obj.values()])
        and all([isinstance(c, str) for d in obj.values() for c, _ in d.items()])
        and all([isinstance(t, str) for d in obj.values() for _, t in d.items()])
    )
    # pylint: enable=g-complex-comprehension


def _isinstance_bird_sample_type(obj: Any) -> bool:
    """Checks if the object is a SQLGlot schema type."""
    return isinstance(obj, dict) and not _isinstance_sqlglot_schema_type(obj)


class SqlTranslator:
    """Translator from SQLite to BigQuery.

    This class is used to translate SQL queries from an input SQL dialect like
    SQLite to an output SQL dialect like BigQuery. It uses the SQLGlot library as
    a tool to perform the translation.

    The translation is done by the following steps:
    1. (Optional) If there are errors in the input SQL query, the input SQL query
       is first modified by the LLM to address the errors.
    2. The input SQL query is then translated to a SQL query in the output SQL
       dialect by the tool.
    3. (Optional) If there are errors in the tool output SQL query, the tool
       output SQL query is modified by the LLM to address the errors.

    Class Attributes:
      INPUT_DIALECT: The input SQL dialect.
      OUTPUT_DIALECT: The output SQL dialect.

    Attributes:
      sql_query: The SQL query to translate.
      model: The model object, or the name of the model to use for the LLM.
      temperature: The temperature to use for the LLM.
      process_input_errors: True if any errors in the input SQL query should be
        processed by the LLM.
      process_tool_output_errors: True if any errors in the tool output SQL query
        should be processed by the LLM.
    """

    INPUT_DIALECT: Final[str] = "sqlite"
    OUTPUT_DIALECT: Final[str] = "bigquery"

    def __init__(
        self,
        model: str | GeminiModel = "gemini-2.5-flash",
        temperature: float = 0.5,
        process_input_errors: bool = False,
        process_tool_output_errors: bool = False,
    ):
        """Initializes the translator."""
        self._process_input_errors: bool = process_input_errors
        self._process_tool_output_errors: bool = process_tool_output_errors
        self._input_errors: str | None = None
        self._tool_output_errors: str | None = None
        self._temperature: float = temperature
        if isinstance(model, str):
            self._model = GeminiModel(model_name=model, temperature=self._temperature)
        else:
            self._model = model

    @classmethod
    def _parse_response(cls, text: str) -> str | None:
        """Extracts the SQL query from the response text."""
        pattern = r"```sql(.*?)```"
        match = re.search(pattern, text, re.DOTALL)
        if match:
            return match.group(1).strip()
        return None

    @classmethod
    def _apply_heuristics(cls, sql_query: str) -> str:
        """Applies heuristics to the SQL query."""
        if "''" in sql_query:
            sql_query = sql_query.replace("''", "\\'")
        return sql_query

    @classmethod
    def _extract_schema_from_ddl_statement(cls, ddl_statement: str) -> TableSchemaType:
        """Extracts the schema from a single DDL statement."""
        # Split the DDL statement into table name and columns.
        # Match the following pattern:
        # CREATE [OR REPLACE] TABLE [`]<table_name>[`] (<all_columns>);
        splitter_pattern = (
            # CREATE [OR REPLACE] TABLE
            r"^\s*CREATE\s+(?:OR\s+REPLACE\s+)?TABLE\s+"
            # Match the table name, optionally surrounded by backticks.
            r"(?:`)?(?P<table_name>[\w\d\-\_\.]+)(?:`)?\s*"
            # Match the column name as everything between the first and last
            # parentheses followed by a semicolon.
            r"\((?P<all_columns>.*)\);$"
        )
        split_match = regex.search(
            splitter_pattern,
            ddl_statement,
            flags=re.DOTALL | re.VERBOSE | re.MULTILINE,
        )
        if not split_match:
            return None, None

        table_name = split_match.group("table_name")
        all_columns = split_match.group("all_columns").strip()
        if not table_name or not all_columns:
            return None, None

        # Extract the columns from the DDL statement.
        # Match the following pattern:
        # <column_name> <column_type> [<ignored_text>]
        # [, <column_name> <column_type> [<ignored_text>]]*
        # Ignore any comments. Ignore any INSERT INTO statements. Ignore any
        # lines beginning with a parenthesis (these are example values).
        column_pattern = (
            # Ignore any comments.
            r"\s*--.*(*SKIP)(*FAIL)"
            # Ignore any INSERT INTO statements.
            r"|\s*INSERT\s+INTO.*(*SKIP)(*FAIL)"
            # Ignore any lines beginning with a parenthesis.
            r"|\s*\(.*(*SKIP)(*FAIL)"
            # Match the column name and type, optionally with backticks.
            r"|\s*(?:`)?\s*(?P<column_name>\w+)(?:`)?\s+(?P<column_type>\w+).*"
        )  # (?:,)?
        columns = regex.findall(column_pattern, all_columns, flags=re.VERBOSE)
        return table_name, columns

    @classmethod
    def extract_schema_from_ddls(cls, ddls: str) -> DDLSchemaType:
        """Extracts the schema from multiple DDL statements."""
        ddl_statements = ddls.split(";\n")
        ddl_statements = [ddl.strip() for ddl in ddl_statements if ddl.strip()]
        schema = []
        for ddl_statement in ddl_statements:
            if ddl_statement:
                ddl_statement = ddl_statement.strip() + ";"  # Add the semicolon back.
                table_name, columns = cls._extract_schema_from_ddl_statement(
                    ddl_statement
                )
                if table_name and columns:
                    schema.append((table_name, columns))
        return schema

    @classmethod
    def _get_schema_from_bird_sample(
        cls, sample: BirdSampleType
    ) -> dict[str, dict[str, str]]:
        """Returns the schema from the Bird dataset example."""
        col_types_map: dict[str, str] = {
            "text": "TEXT",
            "number": "FLOAT",
            "date": "DATE",
            "datetime": "DATETIME",
            "time": "TIME",
            "timestamp": "TIMESTAMP",
            "bool": "BOOL",
        }
        tables = sample["db_table_names"]
        table_ids = sample["db_column_names"]["table_id"][1:]
        column_names = sample["db_column_names"]["column_name"][1:]
        column_types = sample["db_column_types"][1:]
        column_types = [col_types_map[col_type] for col_type in column_types]
        assert len(column_names) == len(column_types)
        cols_and_types: list[tuple[str, str]] = list(zip(column_names, column_types))
        tables_to_columns: dict[str, dict[str, str]] = {}
        for id_pos, table_id in enumerate(table_ids):
            if tables[table_id] in tables_to_columns.keys():
                tables_to_columns[tables[table_id]].update(
                    dict([cols_and_types[id_pos]])
                )
            else:
                tables_to_columns[tables[table_id]] = dict([cols_and_types[id_pos]])
        return tables_to_columns

    @classmethod
    def _get_table_parts(cls, table_name: str) -> tuple[str | None, str | None, str]:
        """Returns the table parts from the table name."""
        table_parts = table_name.split(".")
        if len(table_parts) == 3:
            return table_parts
        elif len(table_parts) == 2:
            return None, *table_parts
        elif len(table_parts) == 1:
            return None, None, *table_parts
        else:
            raise ValueError(f"Invalid table name: {table_name}")

    @classmethod
    def format_schema(cls, schema: DDLSchemaType) -> SQLGlotSchemaType:
        """Formats the DDL schema for use in SQLGlot."""
        schema_dict = {}
        catalog, db = None, None
        for table_name, columns in schema:
            catalog, db, table_name = cls._get_table_parts(table_name)
            schema_dict[table_name] = {}
            for column_name, column_type in columns:
                schema_dict[table_name][column_name] = column_type
        if db:
            schema_dict = {db: schema_dict}
        if catalog:
            schema_dict = {catalog: schema_dict}
        return schema_dict

    @classmethod
    def rewrite_schema_for_sqlglot(
        cls, schema: str | SQLGlotSchemaType | BirdSampleType
    ) -> SQLGlotSchemaType:
        """Rewrites the schema for use in SQLGlot."""
        schema_dict = None
        if schema:
            if isinstance(schema, str):
                schema = cls.extract_schema_from_ddls(schema)
                schema_dict = cls.format_schema(schema)
            elif _isinstance_sqlglot_schema_type(schema):
                schema_dict = schema
            elif _isinstance_bird_sample_type(schema):
                schema_dict = cls._get_schema_from_bird_sample(schema)
            elif _isinstance_ddl_schema_type(schema):
                schema_dict = cls.format_schema(schema)
            else:
                raise TypeError(f"Unsupported schema type: {type(schema)}")
        return schema_dict

    @classmethod
    def _check_for_errors(
        cls,
        sql_query: str,
        sql_dialect: str,
        db: str | None = None,
        catalog: str | None = None,
        schema_dict: SQLGlotSchemaType | None = None,
    ) -> tuple[str | None, str]:
        """Checks for errors in the SQL query.

        Args:
          sql_query: The SQL query to check for errors.
          sql_dialect: The SQL dialect of the SQL query.
          db: The database to use for the translation. This field is optional.
          catalog: The catalog to use for the translation. `catalog` is the SQLGlot
            term for the project ID. This field is optional.
          schema_dict: The DDL schema to use for the translation. The DDL format is
            in the SQLGlot format. This field is optional.

        Returns:
          tuple of the errors in the SQL query, or None if there are no errors, and
          the SQL query after optimization.
        """
        try:
            # First, try to parse the SQL query into a SQLGlot AST.
            sql_query_ast = sqlglot.parse_one(
                sql=sql_query,
                read=sql_dialect.lower(),
                error_level=sqlglot.ErrorLevel.IMMEDIATE,
            )
            # Then add the database and catalog information for each table to the AST.
            for table in sql_query_ast.find_all(sqlglot.exp.Table):
                table.set("catalog", sqlglot.exp.Identifier(this=catalog, quoted=True))
                table.set("db", sqlglot.exp.Identifier(this=db, quoted=True))
            # Then, try to optimize the SQL query.
            sql_query_ast = sqlglot.optimizer.optimize(
                sql_query_ast,
                dialect=sql_dialect.lower(),
                schema=schema_dict,
                db=db,
                catalog=catalog,
                error_level=sqlglot.ErrorLevel.IMMEDIATE,
            )
            sql_query = sql_query_ast.sql(sql_dialect.lower())
        except sqlglot.errors.SqlglotError as e:
            return str(e), sql_query
        return None, sql_query

    def _fix_errors(
        self,
        sql_query: str,
        sql_dialect: str,
        apply_heuristics: bool,
        db: str | None = None,
        catalog: str | None = None,
        ddl_schema: str | SQLGlotSchemaType | BirdSampleType | None = None,
        number_of_candidates: int = 1,
    ) -> str:
        """Fixes errors in the SQL query.

        Args:
          sql_query: The SQL query to fix.
          sql_dialect: The input SQL dialect.
          apply_heuristics: True if the heuristics should be applied.
          db: The database to use for the translation. This field is optional.
          catalog: The catalog to use for the translation. `catalog` is the SQLGlot
            term for the project ID. This field is optional.
          ddl_schema: The DDL schema to use for the translation. The DDL format can
            be the SQLGlot format, the DDL schema format, a Bird dataset example, or
            a string containing multiple DDL statements. This field is optional.
          number_of_candidates: The number of candidates to generate, default is 1.

        Returns:
          str: The fixed SQL query.
        """
        if apply_heuristics:
            sql_query = self._apply_heuristics(sql_query)
        # Reformat the schema if provided. This will remove any comments and
        # `INSERT INTO` statements.
        schema_dict = self.rewrite_schema_for_sqlglot(ddl_schema)
        errors_and_sql: tuple[str | None, str] = self._check_for_errors(
            sql_query=sql_query,
            sql_dialect=self.OUTPUT_DIALECT,
            db=db,
            catalog=catalog,
            schema_dict=schema_dict,
        )
        errors, sql_query = errors_and_sql
        responses = sql_query  # Default to the input SQL query after error check.
        if errors:
            print("Processing input errors")
            if schema_dict:
                # If the schema is provided, then insert it into the prompt.
                schema_insert = f"\nThe database schema is:\n{schema_dict}\n"
            else:
                schema_insert = "\n"
            prompt: str = CORRECTION_PROMPT_TEMPLATE_V1_0.format(
                sql_dialect=sql_dialect.lower(),
                errors=errors,
                sql_query=sql_query,
                schema_insert=schema_insert,
            )
            requests: list[str] = [prompt for _ in range(number_of_candidates)]
            responses: list[str] = self._model.call_parallel(
                requests, parser_func=self._parse_response
            )
            if responses:
                # We only use the first response. Therefore the `number_of_candidates`
                # parameter is not used.
                # pylint: disable=g-bad-todo
                # pylint: enable=g-bad-todo
                # First, find the first non-None response.
                responses = [r for r in responses if r is not None]
                if responses:
                    # Then, return the first non-None response.
                    responses = responses[0]
        return responses

    def translate(
        self,
        sql_query: str,
        db: str | None = None,
        catalog: str | None = None,
        ddl_schema: str | SQLGlotSchemaType | BirdSampleType | None = None,
    ) -> str:
        """Translates the SQL query to the output SQL dialect.

        Args:
          sql_query: The SQL query to translate.
          db: The database to use for the translation. This field is optional.
          catalog: The catalog to use for the translation. `catalog` is the SQLGlot
            term for the project ID. This field is optional.
          ddl_schema: The DDL schema to use for the translation. The DDL format can
            be the SQLGlot format or the DDL schema format. This field is optional.

        Returns:
          The translated SQL query.
        """
        print("****** sql_query at translator entry:", sql_query)
        if self._process_input_errors:
            sql_query = self._fix_errors(
                sql_query,
                db=db,
                catalog=catalog,
                sql_dialect=self.OUTPUT_DIALECT,
                ddl_schema=ddl_schema,
                apply_heuristics=True,
            )
        print("****** sql_query after fix_errors:", sql_query)
        sql_query = sqlglot.transpile(
            sql=sql_query,
            read=self.INPUT_DIALECT,
            write=self.OUTPUT_DIALECT,
            error_level=sqlglot.ErrorLevel.IMMEDIATE,
        )[
            0
        ]  # Transpile returns a list of strings.
        print("****** sql_query after transpile:", sql_query)
        if self._tool_output_errors:
            sql_query = self._fix_errors(
                sql_query,
                db=db,
                catalog=catalog,
                sql_dialect=self.OUTPUT_DIALECT,
                ddl_schema=ddl_schema,
                apply_heuristics=True,
            )

        sql_query = sql_query.strip().replace('"', "`")
        sql_query = self._apply_heuristics(sql_query)

        return sql_query



================================================
FILE: adk-references/agents-tw/data-science/data_science/sub_agents/bqml/__init__.py
================================================
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.




================================================
FILE: adk-references/agents-tw/data-science/data_science/sub_agents/bqml/agent.py
================================================
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""資料科學代理 V2：產生 nl2py 並使用程式碼直譯器 (Code Interpreter) 執行程式碼。"""
import os
from google.adk.agents import Agent
from google.adk.tools import ToolContext
from google.adk.tools.agent_tool import AgentTool
from google.adk.agents.callback_context import CallbackContext


from data_science.sub_agents.bqml.tools import (
    check_bq_models,
    execute_bqml_code,
    rag_response,
)
from .prompts import return_instructions_bqml


from data_science.sub_agents.bigquery.agent import database_agent as bq_db_agent
from data_science.sub_agents.bigquery.tools import (
    get_database_settings as get_bq_database_settings,
)


def setup_before_agent_call(callback_context: CallbackContext):
    """設定代理。"""

    # 在 session.state 中設定資料庫設定
    if "database_settings" not in callback_context.state:
        db_settings = dict()
        db_settings["use_database"] = "BigQuery"
        callback_context.state["all_db_settings"] = db_settings

    # 在指令中設定結構 (schema)
    if callback_context.state["all_db_settings"]["use_database"] == "BigQuery":
        callback_context.state["database_settings"] = get_bq_database_settings()
        schema = callback_context.state["database_settings"]["bq_ddl_schema"]

        callback_context._invocation_context.agent.instruction = (
            return_instructions_bqml()
            + f"""

   </此查詢的 BQML 參考>
    
    <相關資料的 BigQuery 結構 (schema) 及幾筆範例資料列>
    {schema}
    </相關資料的 BigQuery 結構 (schema) 及幾筆範例資料列>
    """
        )


async def call_db_agent(
    question: str,
    tool_context: ToolContext,
):
    """呼叫資料庫 (nl2sql) 代理的工具。"""
    print(
        "\n call_db_agent.use_database:"
        f' {tool_context.state["all_db_settings"]["use_database"]}'
    )
    database_agent = (
        bq_db_agent
        if tool_context.state["all_db_settings"]["use_database"] == "BigQuery"
        # else pg_db_agent
        else None
    )
    agent_tool = AgentTool(agent=database_agent)
    db_agent_output = await agent_tool.run_async(
        args={"request": question}, tool_context=tool_context
    )
    tool_context.state["db_agent_output"] = db_agent_output
    return db_agent_output


root_agent = Agent(
    model=os.getenv("BQML_AGENT_MODEL"),
    name="bq_ml_agent",
    instruction=return_instructions_bqml(),
    before_agent_callback=setup_before_agent_call,
    tools=[execute_bqml_code, check_bq_models, call_db_agent, rag_response],
)



================================================
FILE: adk-references/agents-tw/data-science/data_science/sub_agents/bqml/prompts.py
================================================
[Binary file]


================================================
FILE: adk-references/agents-tw/data-science/data_science/sub_agents/bqml/tools.py
================================================
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import time
import os
from google.cloud import bigquery
from vertexai import rag


def check_bq_models(dataset_id: str) -> str:
    """Lists models in a BigQuery dataset and returns them as a string.

    Args:
        dataset_id: The ID of the BigQuery dataset (e.g., "project.dataset").

    Returns:
        A string representation of a list of dictionaries, where each dictionary
        contains the 'name' and 'type' of a model in the specified dataset.
        Returns an empty string "[]" if no models are found.
    """

    try:
        client = bigquery.Client()

        models = client.list_models(dataset_id)
        model_list = []  # Initialize as a list

        print(f"Models contained in '{dataset_id}':")
        for model in models:
            model_id = model.model_id
            model_type = model.model_type
            model_list.append({"name": model_id, "type": model_type})

        return str(model_list)

    except Exception as e:
        return f"An error occurred: {str(e)}"


def execute_bqml_code(bqml_code: str, project_id: str, dataset_id: str) -> str:
    """
    Executes BigQuery ML code.
    """

    # timeout_seconds = 1500

    client = bigquery.Client(project=project_id)

    try:
        query_job = client.query(bqml_code)
        start_time = time.time()

        while not query_job.done():
            elapsed_time = time.time() - start_time
            # if elapsed_time > timeout_seconds:
            #     return (
            #         "Timeout: BigQuery job did not complete within"
            #         f" {timeout_seconds} seconds. Job ID: {query_job.job_id}"
            #     )

            print(
                f"Query Job Status: {query_job.state}, Elapsed Time:"
                f" {elapsed_time:.2f} seconds. Job ID: {query_job.job_id}"
            )
            time.sleep(5)

        if query_job.error_result:
            return f"Error executing BigQuery ML code: {query_job.error_result}"

        if query_job.exception():
            return f"Exception during BigQuery ML execution: {query_job.exception()}"

        results = query_job.result()
        if results.total_rows > 0:
            result_string = ""
            for row in results:
                result_string += str(dict(row.items())) + "\n"
            return f"BigQuery ML code executed successfully. Results:\n{result_string}"
        else:
            return "BigQuery ML code executed successfully."

    except Exception as e:
        return f"An error occurred: {str(e)}"


def rag_response(query: str) -> str:
    """Retrieves contextually relevant information from a RAG corpus.

    Args:
        query (str): The query string to search within the corpus.

    Returns:
        vertexai.rag.RagRetrievalQueryResponse: The response containing retrieved
        information from the corpus.
    """
    corpus_name = os.getenv("BQML_RAG_CORPUS_NAME")

    rag_retrieval_config = rag.RagRetrievalConfig(
        top_k=3,  # Optional
        filter=rag.Filter(vector_distance_threshold=0.5),  # Optional
    )
    response = rag.retrieval_query(
        rag_resources=[
            rag.RagResource(
                rag_corpus=corpus_name,
            )
        ],
        text=query,
        rag_retrieval_config=rag_retrieval_config,
    )
    return str(response)



================================================
FILE: adk-references/agents-tw/data-science/data_science/utils/create_bq_table.py
================================================
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


import os
from pathlib import Path

from dotenv import load_dotenv
from google.cloud import bigquery

# 定義 .env 檔案的路徑
env_file_path = Path(__file__).parent.parent.parent / ".env"
print(env_file_path)

# 從指定的 .env 檔案載入環境變數
load_dotenv(dotenv_path=env_file_path)


def load_csv_to_bigquery(data_project_id,
                         dataset_name,
                         table_name,
                         csv_filepath):
    """將 CSV 檔案載入至 BigQuery 資料表。

    Args:
        data_project_id: 用於 BQ 資料的 GCP 專案。
        dataset_name: BigQuery 資料集的名稱。
        table_name: BigQuery 資料表的名稱。
        csv_filepath: CSV 檔案的路徑。
    """

    client = bigquery.Client(project=data_project_id)

    dataset_ref = client.dataset(dataset_name)
    table_ref = dataset_ref.table(table_name)

    job_config = bigquery.LoadJobConfig(
        source_format=bigquery.SourceFormat.CSV,
        skip_leading_rows=1,  # 略過標頭列
        autodetect=True,  # 自動偵測結構
    )

    with open(csv_filepath, "rb") as source_file:
        job = client.load_table_from_file(
            source_file, table_ref, job_config=job_config
        )

    job.result()  # 等待工作完成

    print(f"已載入 {job.output_rows} 列至 "
          f"{dataset_name}.{table_name}")


def create_dataset_if_not_exists(compute_project_id,
                                 data_project_id,
                                 dataset_name):
    """如果 BigQuery 資料集不存在，則建立它。

    Args:
        compute_project_id: 用於 BQ 運算的 GCP 專案。
        data_project_id: 用於 BQ 資料的 GQP 專案。
        dataset_name: BigQuery 資料集的名稱。
    """
    client = bigquery.Client(project=compute_project_id)
    dataset_full_name = f"{data_project_id}.{dataset_name}"

    try:
        client.get_dataset(dataset_full_name)  # 發出 API 請求。
        print(f"資料集 {dataset_full_name} 已存在")
    except Exception:
        dataset = bigquery.Dataset(dataset_full_name)
        dataset.location = "US"  # 設定位置 (例如 "US", "EU")
        dataset = client.create_dataset(dataset, timeout=30)  # 發出 API 請求。
        print(f"已建立資料集 {dataset_full_name}")


def main():

    current_directory = os.getcwd()
    print(f"目前工作目錄：{current_directory}")

    """將 CSV 檔案載入 BigQuery 的主要函式。"""
    data_project_id = os.getenv("BQ_DATA_PROJECT_ID")
    compute_project_id = os.getenv("BQ_COMPUTE_PROJECT_ID")
    if not data_project_id:
        raise ValueError("尚未設定 BQ_DATA_PROJECT_ID 環境變數。")
    if not compute_project_id:
        raise ValueError("尚未設定 BQ_COMPUTE_PROJECT_ID 環境變數。")

    dataset_name = "forecasting_sticker_sales"
    train_csv_filepath = "data_science/utils/data/train.csv"
    test_csv_filepath = "data_science/utils/data/test.csv"

    # 如果資料集不存在，則建立它
    print("正在建立資料集。")
    create_dataset_if_not_exists(compute_project_id,
                                 data_project_id,
                                 dataset_name)

    # 載入訓練資料
    print("正在載入訓練資料表。")
    load_csv_to_bigquery(data_project_id,
                         dataset_name,
                         "train",
                         train_csv_filepath)

    # 載入測試資料
    print("正在載入測試資料表。")
    load_csv_to_bigquery(data_project_id,
                         dataset_name,
                         "test",
                         test_csv_filepath)


if __name__ == "__main__":
    main()



================================================
FILE: adk-references/agents-tw/data-science/data_science/utils/reference_guide_RAG.py
================================================
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
from pathlib import Path
from dotenv import load_dotenv, set_key
import vertexai
from vertexai import rag


# 定義 .env 檔案的路徑
env_file_path = Path(__file__).parent.parent.parent / ".env"
print(env_file_path)

# 從指定的 .env 檔案載入環境變數
load_dotenv(dotenv_path=env_file_path)

PROJECT_ID = os.getenv("GOOGLE_CLOUD_PROJECT")
corpus_name = os.getenv("BQML_RAG_CORPUS_NAME")

display_name = "bqml_referenceguide_corpus"

paths = [
    "gs://cloud-samples-data/adk-samples/data-science/bqml"
]  # 支援 Google Cloud Storage 和 Google Drive 連結


# 每個會話僅初始化一次 Vertex AI API
vertexai.init(project=PROJECT_ID, location="us-central1")


def create_RAG_corpus():
    # 建立 RagCorpus
    # 設定嵌入模型，例如 "text-embedding-005"。
    embedding_model_config = rag.RagEmbeddingModelConfig(
        vertex_prediction_endpoint=rag.VertexPredictionEndpoint(
            publisher_model="publishers/google/models/text-embedding-005"
        )
    )

    backend_config = rag.RagVectorDbConfig(
        rag_embedding_model_config=embedding_model_config
    )

    bqml_corpus = rag.create_corpus(
        display_name=display_name,
        backend_config=backend_config,
    )

    write_to_env(bqml_corpus.name)

    return bqml_corpus.name


def ingest_files(corpus_name):

    transformation_config = rag.TransformationConfig(
        chunking_config=rag.ChunkingConfig(
            chunk_size=512,
            chunk_overlap=100,
        ),
    )

    rag.import_files(
        corpus_name,
        paths,
        transformation_config=transformation_config,  # 可選
        max_embedding_requests_per_min=1000,  # 可選
    )

    # 列出 rag 語料庫中的檔案
    rag.list_files(corpus_name)


def rag_response(query: str) -> str:
    """從 RAG 語料庫中擷取與上下文相關的資訊。

    Args:
        query (str): 要在語料庫中搜尋的查詢字串。

    Returns:
        vertexai.rag.RagRetrievalQueryResponse: 包含從語料庫中擷取資訊的回應。
    """
    corpus_name = os.getenv("BQML_RAG_CORPUS_NAME")

    rag_retrieval_config = rag.RagRetrievalConfig(
        top_k=3,  # 可選
        filter=rag.Filter(vector_distance_threshold=0.5),  # 可選
    )
    response = rag.retrieval_query(
        rag_resources=[
            rag.RagResource(
                rag_corpus=corpus_name,
            )
        ],
        text=query,
        rag_retrieval_config=rag_retrieval_config,
    )
    return str(response)


def write_to_env(corpus_name):
    """將語料庫名稱寫入指定的 .env 檔案。

    Args:
        corpus_name: 要寫入的語料庫名稱。
    """

    load_dotenv(env_file_path)  # 載入任何現有的變數

    # 在 .env 檔案中設定鍵值對
    set_key(env_file_path, "BQML_RAG_CORPUS_NAME", corpus_name)
    print(f"已將 BQML_RAG_CORPUS_NAME '{corpus_name}' 寫入 {env_file_path}")


if __name__ == "__main__":
    # rag_corpus = rag.list_corpora()

    corpus_name = os.getenv("BQML_RAG_CORPUS_NAME")

    print("正在建立語料庫。")
    corpus_name = create_RAG_corpus()
    print(f"語料庫名稱：{corpus_name}")

    print(f"正在將檔案匯入語料庫：{corpus_name}")
    ingest_files(corpus_name)
    print(f"已將檔案匯入語料庫：{corpus_name}")



================================================
FILE: adk-references/agents-tw/data-science/data_science/utils/utils.py
================================================
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import json
import os

from vertexai.preview.extensions import Extension


def list_all_extensions():
  extensions = Extension.list(location='us-central1')
  for extension in extensions:
    print('名稱:', extension.gca_resource.name)
    print('顯示名稱:', extension.gca_resource.display_name)
    print('描述:', extension.gca_resource.description)


def get_env_var(var_name):
  """擷取環境變數的值。

  Args:
    var_name: 環境變數的名稱。

  Returns:
    環境變數的值，如果未設定則為 None。

  Raises:
    ValueError: 如果未設定環境變數。
  """
  try:
    value = os.environ[var_name]
    return value
  except KeyError:
    raise ValueError(f'缺少環境變數：{var_name}')


def get_image_bytes(filepath):
  """讀取圖片檔案並回傳其位元組。

  Args:
    filepath: 圖片檔案的路徑。

  Returns:
    圖片檔案的位元組，如果檔案不存在或無法讀取，則為 None。
  """
  try:
    with open(filepath, 'rb') as f:  # "rb" 模式用於二進位讀取
      image_bytes = f.read()
    return image_bytes
  except FileNotFoundError:
    print(f'錯誤：在 {filepath} 找不到檔案')
    return None
  except Exception as e:
    print(f'讀取檔案時發生錯誤：{e}')
    return None


def extract_json_from_model_output(model_output):
  """從可能包含 markdown 程式碼區塊的字串中擷取 JSON 物件。

  Args:
    model_output: 一個可能包含包在 markdown 程式碼區塊 (```json ... ```) 中的 JSON 物件的字串。

  Returns:
    一個代表擷取的 JSON 物件的 Python 字典，
    如果 JSON 擷取失敗，則為 None。
  """
  try:
    cleaned_output = (
        model_output.replace('```json', '').replace('```', '').strip()
    )
    json_object = json.loads(cleaned_output)
    return json_object
  except json.JSONDecodeError as e:
    msg = f'解碼 JSON 時發生錯誤：{e}'
    print(msg)
    return {'error': msg}


if __name__ == '__main__':
  list_all_extensions()



================================================
FILE: adk-references/agents-tw/data-science/data_science/utils/data/test.csv
================================================
id,date,country,store,product,num_sold
229681,27/12/2016,Canada,Discount Stickers,Kaggle,694
229682,27/12/2016,Canada,Discount Stickers,Kaggle Tiers,577
229683,27/12/2016,Canada,Discount Stickers,Kerneler,298


================================================
FILE: adk-references/agents-tw/data-science/data_science/utils/data/train.csv
================================================
id,date,country,store,product,num_sold
0,01/01/2010,Canada,Discount Stickers,Holographic Goose,
1,01/01/2010,Canada,Discount Stickers,Kaggle,973
2,01/01/2010,Canada,Discount Stickers,Kaggle Tiers,906


================================================
FILE: adk-references/agents-tw/data-science/deployment/__init__.py
================================================
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.




================================================
FILE: adk-references/agents-tw/data-science/deployment/deploy.py
================================================
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""資料科學代理的部署腳本。"""

import logging
import os

import vertexai
from absl import app, flags
from data_science.agent import root_agent
from dotenv import load_dotenv
from google.api_core import exceptions as google_exceptions
from google.cloud import storage
from vertexai import agent_engines
from vertexai.preview.reasoning_engines import AdkApp

FLAGS = flags.FLAGS
flags.DEFINE_string("project_id", None, "GCP 專案 ID。")
flags.DEFINE_string("location", None, "GCP 位置。")
flags.DEFINE_string(
    "bucket", None, "GCP 儲存桶名稱（不含 gs:// 前綴）。"
)  # 更改了旗標描述
flags.DEFINE_string("resource_id", None, "ReasoningEngine 資源 ID。")

flags.DEFINE_bool("create", False, "建立新代理。")
flags.DEFINE_bool("delete", False, "刪除現有代理。")
flags.mark_bool_flags_as_mutual_exclusive(["create", "delete"])

AGENT_WHL_FILE = "data_science-0.1-py3-none-any.whl"

# 設定日誌記錄
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def setup_staging_bucket(
    project_id: str, location: str, bucket_name: str
) -> str:
    """
    檢查暫存儲存桶是否存在，如果不存在則建立。

    Args:
        project_id: GCP 專案 ID。
        location: 儲存桶的 GCP 位置。
        bucket_name: 儲存桶的預期名稱（不含 gs:// 前綴）。

    Returns:
        完整的儲存桶路徑 (gs://<bucket_name>)。

    Raises:
        google_exceptions.GoogleCloudError: 如果儲存桶建立失敗。
    """
    storage_client = storage.Client(project=project_id)
    try:
        # 檢查儲存桶是否存在
        bucket = storage_client.lookup_bucket(bucket_name)
        if bucket:
            logger.info("暫存儲存桶 gs://%s 已存在。", bucket_name)
        else:
            logger.info(
                "找不到暫存儲存桶 gs://%s。正在建立...", bucket_name
            )
            # 如果儲存桶不存在則建立
            new_bucket = storage_client.create_bucket(
                bucket_name, project=project_id, location=location
            )
            logger.info(
                "已在 %s 成功建立暫存儲存桶 gs://%s。",
                new_bucket.name,
                location,
            )
            # 為求簡單，啟用統一的儲存桶層級存取權
            new_bucket.iam_configuration.uniform_bucket_level_access_enabled = (
                True
            )
            new_bucket.patch()
            logger.info(
                "已為 gs://%s 啟用統一的儲存桶層級存取權。",
                new_bucket.name,
            )

    except google_exceptions.Forbidden as e:
        logger.error(
            (
                "儲存桶 gs://%s 的權限遭拒錯誤。"
                "請確保服務帳號具有「儲存空間管理員」角色。錯誤：%s"
            ),
            bucket_name,
            e,
        )
        raise
    except google_exceptions.Conflict as e:
        logger.warning(
            (
                "儲存桶 gs://%s 可能已存在但由其他專案擁有或最近已刪除。錯誤：%s"
            ),
            bucket_name,
            e,
        )
        # 假設如果它存在，即使有衝突警告也可以繼續
    except google_exceptions.ClientError as e:
        logger.error(
            "建立或存取儲存桶 gs://%s 失敗。錯誤：%s",
            bucket_name,
            e,
        )
        raise

    return f"gs://{bucket_name}"


def create(env_vars: dict[str, str]) -> None:
    """建立並部署代理。"""
    adk_app = AdkApp(
        agent=root_agent,
        enable_tracing=False,
    )

    if not os.path.exists(AGENT_WHL_FILE):
        logger.error("在以下位置找不到代理 wheel 檔案：%s", AGENT_WHL_FILE)
        # 考慮在此處新增有關如何建置 wheel 檔案的說明
        raise FileNotFoundError(f"找不到代理 wheel 檔案：{AGENT_WHL_FILE}")

    logger.info("正在使用代理 wheel 檔案：%s", AGENT_WHL_FILE)

    remote_agent = agent_engines.create(
        adk_app,
        requirements=[AGENT_WHL_FILE],
        extra_packages=[AGENT_WHL_FILE],
        env_vars=env_vars
    )
    logger.info("已建立遠端代理：%s", remote_agent.resource_name)
    print(f"\n已成功建立代理：{remote_agent.resource_name}")


def delete(resource_id: str) -> None:
    """刪除指定的代理。"""
    logger.info("正在嘗試刪除代理：%s", resource_id)
    try:
        remote_agent = agent_engines.get(resource_id)
        remote_agent.delete(force=True)
        logger.info("已成功刪除遠端代理：%s", resource_id)
        print(f"\n已成功刪除代理：{resource_id}")
    except google_exceptions.NotFound:
        logger.error("找不到資源 ID 為 %s 的代理。", resource_id)
        print(f"\n找不到代理 {resource_id}。")
        print(f"\n找不到代理：{resource_id}")
    except Exception as e:
        logger.error(
            "刪除代理 %s 時發生錯誤：%s", resource_id, e
        )
        print(f"\n刪除代理 {resource_id} 時發生錯誤：{e}")


def main(argv: list[str]) -> None:  # pylint: disable=unused-argument
    """主要執行函式。"""
    load_dotenv()
    env_vars = {}

    project_id = (
        FLAGS.project_id
        if FLAGS.project_id
        else os.getenv("GOOGLE_CLOUD_PROJECT")
    )
    location = (
        FLAGS.location if FLAGS.location else os.getenv("GOOGLE_CLOUD_LOCATION")
    )
    # 如果未提供，則預設儲存桶名稱慣例
    default_bucket_name = f"{project_id}-adk-staging" if project_id else None
    bucket_name = (
        FLAGS.bucket
        if FLAGS.bucket
        else os.getenv("GOOGLE_CLOUD_STORAGE_BUCKET", default_bucket_name)
    )
    # 部署到 Agent Engine 時，請勿設定 "GOOGLE_CLOUD_PROJECT" 或 "GOOGLE_CLOUD_LOCATION"。
    # 這些是由後端設定的。
    env_vars["ROOT_AGENT_MODEL"] = os.getenv("ROOT_AGENT_MODEL")
    env_vars["ANALYTICS_AGENT_MODEL"] = os.getenv("ANALYTICS_AGENT_MODEL")
    env_vars["BASELINE_NL2SQL_MODEL"] = os.getenv("BASELINE_NL2SQL_MODEL")
    env_vars["BIGQUERY_AGENT_MODEL"] = os.getenv("BIGQUERY_AGENT_MODEL")
    env_vars["BQML_AGENT_MODEL"] = os.getenv("BQML_AGENT_MODEL")
    env_vars["CHASE_NL2SQL_MODEL"] = os.getenv("CHASE_NL2SQL_MODEL")
    env_vars["BQ_DATASET_ID"] = os.getenv("BQ_DATASET_ID")
    env_vars["BQ_DATA_PROJECT_ID"] = os.getenv("BQ_DATA_PROJECT_ID")
    env_vars["BQ_COMPUTE_PROJECT_ID"] = os.getenv("BQ_COMPUTE_PROJECT_ID")
    env_vars["BQML_RAG_CORPUS_NAME"] = os.getenv("BQML_RAG_CORPUS_NAME")
    env_vars["CODE_INTERPRETER_EXTENSION_NAME"] = os.getenv(
        "CODE_INTERPRETER_EXTENSION_NAME")
    env_vars["NL2SQL_METHOD"] = os.getenv("NL2SQL_METHOD")

    logger.info("正在使用專案：%s", project_id)
    logger.info("正在使用位置：%s", location)
    logger.info("正在使用儲存桶名稱：%s", bucket_name)

    # --- 輸入驗證 ---
    if not project_id:
        print("\n錯誤：缺少必要的 GCP 專案 ID。")
        print(
            "請設定 GOOGLE_CLOUD_PROJECT 環境變數或使用 --project_id 旗標。"
        )
        return
    if not location:
        print("\n錯誤：缺少必要的 GCP 位置。")
        print(
            "請設定 GOOGLE_CLOUD_LOCATION 環境變數或使用 --location 旗標。"
        )
        return
    if not bucket_name:
        print("\n錯誤：缺少必要的 GCS 儲存桶名稱。")
        print(
            "請設定 GOOGLE_CLOUD_STORAGE_BUCKET 環境變數或使用 --bucket 旗標。"
        )
        return
    if not FLAGS.create and not FLAGS.delete:
        print("\n錯誤：您必須指定 --create 或 --delete 旗標。")
        return
    if FLAGS.delete and not FLAGS.resource_id:
        print(
            "\n錯誤：使用 --delete 旗標時需要 --resource_id。"
        )
        return
    # --- 結束輸入驗證 ---

    try:
        # 設定暫存儲存桶
        staging_bucket_uri=None
        if FLAGS.create:
            staging_bucket_uri = setup_staging_bucket(
                project_id, location, bucket_name
            )

        # 在儲存桶設定和驗證後初始化 Vertex AI
        vertexai.init(
            project=project_id,
            location=location,
            staging_bucket=staging_bucket_uri,  # 暫存儲存桶現在直接傳遞給 create/update 方法
        )

        if FLAGS.create:
            create(env_vars)
        elif FLAGS.delete:
            delete(FLAGS.resource_id)

    except google_exceptions.Forbidden as e:
        print(
            "權限錯誤：請確保服務帳號/使用者具有必要的權限（例如，儲存空間管理員、Vertex AI 使用者）。"
            f"\n詳細資訊：{e}"
        )
    except FileNotFoundError as e:
        print(f"\n檔案錯誤：{e}")
        print(
            "請確保代理 wheel 檔案存在於 'deployment' 目錄中，且您已執行建置腳本"
            "（例如，poetry build --format=wheel --output=deployment'）。"
        )
    except Exception as e:
        print(f"\n發生未預期的錯誤：{e}")
        logger.exception(
            "main 中未處理的例外狀況："
        )  # 記錄完整的追蹤


if __name__ == "__main__":

    app.run(main)



================================================
FILE: adk-references/agents-tw/data-science/deployment/test_deployment.py
================================================
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""測試資料科學代理部署至 Agent Engine。"""

import asyncio
import os

import vertexai
from absl import app, flags
from dotenv import load_dotenv
from google.adk.sessions import VertexAiSessionService
from vertexai import agent_engines

FLAGS = flags.FLAGS

flags.DEFINE_string("project_id", None, "GCP 專案 ID。")
flags.DEFINE_string("location", None, "GCP 位置。")
flags.DEFINE_string("bucket", None, "GCP 儲存桶。")
flags.DEFINE_string(
    "resource_id",
    None,
    "ReasoningEngine 資源 ID（部署代理後回傳）",
)
flags.DEFINE_string("user_id", None, "使用者 ID（可以是任何字串）。")
flags.mark_flag_as_required("resource_id")
flags.mark_flag_as_required("user_id")


def main(argv: list[str]) -> None:  # pylint: disable=unused-argument

    load_dotenv()

    project_id = (
        FLAGS.project_id
        if FLAGS.project_id
        else os.getenv("GOOGLE_CLOUD_PROJECT")
    )
    location = (
        FLAGS.location if FLAGS.location else os.getenv("GOOGLE_CLOUD_LOCATION")
    )
    bucket = (
        FLAGS.bucket
        if FLAGS.bucket
        else os.getenv("GOOGLE_CLOUD_STORAGE_BUCKET")
    )

    project_id = os.getenv("GOOGLE_CLOUD_PROJECT")
    location = os.getenv("GOOGLE_CLOUD_LOCATION")
    bucket = os.getenv("GOOGLE_CLOUD_STORAGE_BUCKET")

    if not project_id:
        print("缺少必要的環境變數：GOOGLE_CLOUD_PROJECT")
        return
    elif not location:
        print("缺少必要的環境變數：GOOGLE_CLOUD_LOCATION")
        return
    elif not bucket:
        print(
            "缺少必要的環境變數：GOOGLE_CLOUD_STORAGE_BUCKET"
        )
        return

    vertexai.init(
        project=project_id,
        location=location,
        staging_bucket=f"gs://{bucket}",
    )

    session_service = VertexAiSessionService(project_id, location)
    session = asyncio.run(session_service.create_session(
        app_name=FLAGS.resource_id,
        user_id=FLAGS.user_id)
    )

    agent = agent_engines.get(FLAGS.resource_id)
    print(f"找到資源 ID 為 {FLAGS.resource_id} 的代理")

    print(f"為使用者 ID {FLAGS.user_id} 建立工作階段")
    print("輸入 'quit' 離開。")
    while True:
        user_input = input("輸入：")
        if user_input == "quit":
            break

        for event in agent.stream_query(
            user_id=FLAGS.user_id,
            session_id=session.id,
            message=user_input
        ):
            if "content" in event:
                if "parts" in event["content"]:
                    parts = event["content"]["parts"]
                    for part in parts:
                        if "text" in part:
                            text_part = part["text"]
                            print(f"回應：{text_part}")

    asyncio.run(session_service.delete_session(
        app_name=FLAGS.resource_id,
        user_id=FLAGS.user_id,
        session_id=session.id
    ))
    print(f"已刪除使用者 ID {FLAGS.user_id} 的工作階段")


if __name__ == "__main__":
    app.run(main)



================================================
FILE: adk-references/agents-tw/data-science/eval/__init__.py
================================================
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.




================================================
FILE: adk-references/agents-tw/data-science/eval/test_eval.py
================================================
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os

import pytest
from dotenv import find_dotenv, load_dotenv
from google.adk.evaluation.agent_evaluator import AgentEvaluator

pytest_plugins = ("pytest_asyncio",)


@pytest.fixture(scope="session", autouse=True)
def load_env():
    load_dotenv(find_dotenv(".env"))


@pytest.mark.asyncio
async def test_eval_simple():
    """透過會話檔案測試代理的基本能力。"""
    await AgentEvaluator.evaluate(
        "data_science",
        os.path.join(os.path.dirname(__file__), "eval_data/simple.test.json"),
        num_runs=1,
    )



================================================
FILE: adk-references/agents-tw/data-science/eval/eval_data/simple.test.json
================================================
[Binary file]


================================================
FILE: adk-references/agents-tw/data-science/eval/eval_data/test_config.json
================================================
{
  "criteria": {
    "tool_trajectory_avg_score": 1.0,
    "response_match_score": 0.1
  }
}



================================================
FILE: adk-references/agents-tw/data-science/tests/test_agents.py
================================================
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""分析代理及其子代理的測試案例。"""

import os
import sys
import pytest
import unittest

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from google.genai import types
from google.adk.artifacts import InMemoryArtifactService
from google.adk.runners import Runner
from google.adk.sessions import InMemorySessionService

from data_science.agent import root_agent
from data_science.sub_agents.bqml.agent import root_agent as bqml_agent
from data_science.sub_agents.bigquery.agent import database_agent

session_service = InMemorySessionService()
artifact_service = InMemoryArtifactService()


class TestAgents(unittest.TestCase):
    """分析代理及其子代理的測試案例。"""

    def setUp(self):
        """為測試方法進行設定。"""
        self.session = session_service.create_session(
            app_name="DataAgent",
            user_id="test_user",
        )
        self.user_id = "test_user"
        self.session_id = self.session.id

        self.runner = Runner(
            app_name="DataAgent",
            agent=None,
            artifact_service=artifact_service,
            session_service=session_service,
        )

    def _run_agent(self, agent, query):
        """執行代理並取得最終回應的輔助方法。"""
        self.runner.agent = agent
        content = types.Content(role="user", parts=[types.Part(text=query)])
        events = list(
            self.runner.run(
                user_id=self.user_id, session_id=self.session_id, new_message=content
            )
        )

        last_event = events[-1]
        final_response = "".join(
            [part.text for part in last_event.content.parts if part.text]
        )
        return final_response


    @pytest.mark.db_agent
    def test_db_agent_can_handle_env_query(self):
        """使用來自環境變數的查詢測試 db_agent。"""
        query = "what countries exist in the train table?"
        response = self._run_agent(database_agent, query)
        print(response)
        # self.assertIn("Canada", response)
        self.assertIsNotNone(response)

    @pytest.mark.ds_agent
    def test_ds_agent_can_be_called_from_root(self):
        """從根代理測試 ds_agent。"""
        query = "plot the most selling category"
        response = self._run_agent(root_agent, query)
        print(response)
        self.assertIsNotNone(response)

    @pytest.mark.bqml
    def test_bqml_agent_can_check_for_models(self):
        """測試 bqml_agent 是否可以檢查現有模型。"""
        query = "Are there any existing models in the dataset?"
        response = self._run_agent(bqml_agent, query)
        print(response)
        self.assertIsNotNone(response)

    @pytest.mark.bqml
    def test_bqml_agent_can_execute_code(self):
        """測試 bqml_agent 是否可以執行 BQML 程式碼。"""
        query = """
    I want to train a BigQuery ML model on the sales_train_validation data for sales prediction.
    Please show me an execution plan. 
    """
        response = self._run_agent(bqml_agent, query)
        print(response)
        self.assertIsNotNone(response)


if __name__ == "__main__":
    unittest.main()

    # testagent = TestAgents
    # testagent.setUp(testagent)
    # testagent.test_root_agent_can_list_tools(testagent)
    # testagent.test_db_agent_can_handle_env_query(testagent)


