# SRE Assistant æ ¸å¿ƒåƒè€ƒè³‡æºæ·±åº¦åˆ†æ

ä½œç‚º Google ADK é¦–å¸­æ¶æ§‹å¸«ï¼Œå¾ SRE Assistant å¯¦éš›éœ€æ±‚å‡ºç™¼ï¼Œç²¾é¸æœ€ç›¸é—œçš„åƒè€ƒç¯„ä¾‹é€²è¡Œæ·±å…¥åˆ†æã€‚

## 1. RAG å¼•ç”¨ç³»çµ± - æœ€é—œéµåƒè€ƒ

### æ ¸å¿ƒåƒè€ƒæª”æ¡ˆ
- **ä¸»æª”æ¡ˆ**: `docs/references/adk-samples-agents/RAG/agent.py`
- **å·¥å…·å¯¦ç¾**: `docs/references/adk-samples-agents/RAG/tools.py`
- **è©•ä¼°æ¡†æ¶**: `docs/references/adk-samples-agents/RAG/eval/test_eval.py`

### æ·±åº¦å€Ÿé¡åˆ†æ

#### A. å¼•ç”¨æ ¼å¼æ¨™æº–åŒ–å¯¦ç¾
```python
# ä¾†è‡ª RAG/agent.py ç¬¬ 45-67 è¡Œçš„å¼•ç”¨æ¨¡å¼
class SRECitationFormatter:
    """åŸºæ–¼ RAG ç¯„ä¾‹æ”¹é€ çš„ SRE å°ˆç”¨å¼•ç”¨æ ¼å¼"""
    
    def format_diagnostic_citation(self, sources: List[Dict]) -> str:
        """è¨ºæ–·å ±å‘Šçš„å¼•ç”¨æ ¼å¼"""
        citations = []
        
        # å€Ÿé¡ RAG çš„ URL æ ¼å¼åŒ–ï¼Œæ”¹ç‚º SRE è³‡æºå®šä½
        for idx, source in enumerate(sources, 1):
            if source['type'] == 'prometheus_rule':
                # æ ¼å¼ï¼šè¦å‰‡åç¨± (æª”æ¡ˆ:è¡Œè™Ÿ) [æ™‚é–“ç¯„åœ]
                citation = f"{source['rule_name']} ({source['file']}:L{source['line']}) [{source['time_range']}]"
            elif source['type'] == 'runbook':
                # æ ¼å¼ï¼šRunbook æ¨™é¡Œ > ç« ç¯€ (URL)
                citation = f"{source['title']} > {source['section']} ({source['url']})"
            elif source['type'] == 'incident_history':
                # æ ¼å¼ï¼šIncident #ID - ç›¸ä¼¼åº¦% (æ—¥æœŸ)
                citation = f"Incident #{source['id']} - {source['similarity']}% match ({source['date']})"
            
            citations.append(f"[{idx}] {citation}")
        
        return "\n".join(citations)
```

#### B. RAG æª¢ç´¢ç­–ç•¥å„ªåŒ–
```python
# åŸºæ–¼ RAG/tools.py çš„ retrieve_context æ”¹é€ 
class SREContextRetriever:
    """SRE å°ˆç”¨çš„ä¸Šä¸‹æ–‡æª¢ç´¢å™¨"""
    
    def __init__(self):
        # å€Ÿé¡ RAG çš„å¤šç´šæª¢ç´¢ç­–ç•¥
        self.retrieval_stages = [
            ('exact_match', self.search_exact_error),      # ç²¾ç¢ºéŒ¯èª¤ç¢¼åŒ¹é…
            ('semantic', self.search_semantic_similar),    # èªç¾©ç›¸ä¼¼
            ('temporal', self.search_temporal_pattern),    # æ™‚é–“æ¨¡å¼åŒ¹é…
            ('causal', self.search_causal_chain)          # å› æœéˆåˆ†æ
        ]
    
    async def retrieve_diagnostic_context(
        self, 
        error_msg: str, 
        service: str,
        time_window: str = "1h"
    ) -> List[Dict]:
        """å¤šéšæ®µè¨ºæ–·ä¸Šä¸‹æ–‡æª¢ç´¢"""
        all_results = []
        
        for stage_name, search_func in self.retrieval_stages:
            results = await search_func(error_msg, service, time_window)
            
            # å€Ÿé¡ RAG çš„ç›¸é—œæ€§è©•åˆ†æ©Ÿåˆ¶
            scored_results = self._score_relevance(results, error_msg)
            
            # åªä¿ç•™é«˜ç›¸é—œæ€§çµæœ
            filtered = [r for r in scored_results if r['score'] > 0.7]
            all_results.extend(filtered)
            
            # å¦‚æœæ‰¾åˆ°é«˜ç½®ä¿¡åº¦ç­”æ¡ˆï¼Œæå‰è¿”å›
            if any(r['score'] > 0.95 for r in filtered):
                break
        
        # å»é‡ä¸¦æ’åº
        return self._deduplicate_and_rank(all_results)
```

## 2. Software Bug Assistant - GitHub æ•´åˆæœ€ä½³å¯¦è¸

### æ ¸å¿ƒåƒè€ƒæª”æ¡ˆ
- **ä¸»ä»£ç†**: `docs/references/adk-samples-agents/software-bug-assistant/agent.py`
- **GitHub å·¥å…·**: `docs/references/adk-samples-agents/software-bug-assistant/tools/github_tools.py`
- **è³‡æ–™åº«æ•´åˆ**: `docs/references/adk-samples-agents/software-bug-assistant/tools/database_tools.py`

### æ·±åº¦å€Ÿé¡åˆ†æ

#### A. äº‹ä»¶è¿½è¹¤ç³»çµ±å¯¦ç¾
```python
# åŸºæ–¼ software-bug-assistant/tools/github_tools.py æ”¹é€ 
class SREIncidentGitHubTracker:
    """SRE äº‹ä»¶çš„ GitHub æ•´åˆ"""
    
    def __init__(self):
        self.github_client = GitHubClient()
        # å€Ÿé¡ bug assistant çš„æ¨™ç±¤ç³»çµ±
        self.label_mapping = {
            'P0': ['critical', 'production-down', 'sre-p0'],
            'P1': ['high-priority', 'degraded-service', 'sre-p1'],
            'P2': ['medium-priority', 'improvement', 'sre-p2']
        }
    
    async def create_incident_issue(self, incident: SREIncident) -> str:
        """å‰µå»ºäº‹ä»¶ Issue - å€Ÿé¡ bug assistant çš„çµæ§‹åŒ–æ¨¡æ¿"""
        
        # ä½¿ç”¨ bug assistant çš„ Markdown æ¨¡æ¿æ¨¡å¼
        issue_body = f"""
## ğŸš¨ Incident Summary
**Incident ID**: {incident.id}
**Severity**: {incident.severity}
**Service**: {incident.service}
**Start Time**: {incident.start_time}
**MTTR Target**: {self._get_mttr_target(incident.severity)}

## ğŸ“Š Impact Assessment
- **Users Affected**: {incident.users_affected}
- **Revenue Impact**: ${incident.revenue_impact}
- **SLO Violation**: {incident.slo_violation}%

## ğŸ” Diagnosis
{incident.diagnosis}

### Error Logs
```
{incident.error_logs[:500]}  # é™åˆ¶é•·åº¦ï¼Œå¦‚ bug assistant
```

## ğŸ”§ Resolution Steps
{self._format_resolution_steps(incident.resolution_steps)}

## ğŸ“ Follow-up Actions
- [ ] Update runbook
- [ ] Add monitoring for: {incident.monitoring_gap}
- [ ] Schedule postmortem (Due: {incident.postmortem_due_date})

---
*Auto-generated by SRE Assistant at {datetime.now().isoformat()}*
        """
        
        # å‰µå»º Issue ä¸¦è‡ªå‹•åˆ†é…
        issue = await self.github_client.create_issue(
            title=f"[{incident.severity}] {incident.service}: {incident.title}",
            body=issue_body,
            labels=self.label_mapping[incident.severity],
            assignees=self._get_oncall_engineers(incident.service)
        )
        
        # å€Ÿé¡ bug assistant çš„è‡ªå‹•é€£çµåŠŸèƒ½
        await self._link_related_issues(issue.id, incident)
        
        return issue.url
```

#### B. MCP è³‡æ–™åº«å·¥å…·æ•´åˆ
```python
# åŸºæ–¼ software-bug-assistant/tools/database_tools.py çš„å®‰å…¨æŸ¥è©¢æ¨¡å¼
class SREMetricsQueryBuilder:
    """å®‰å…¨çš„ SRE æŒ‡æ¨™æŸ¥è©¢æ§‹å»ºå™¨"""
    
    def __init__(self):
        # å€Ÿé¡ bug assistant çš„æŸ¥è©¢æ¨¡æ¿åŒ–
        self.query_templates = {
            'error_rate': """
                SELECT 
                    timestamp_trunc(timestamp, MINUTE) as minute,
                    service_name,
                    COUNT(CASE WHEN status_code >= 500 THEN 1 END) * 100.0 / COUNT(*) as error_rate
                FROM `{project}.{dataset}.{table}`
                WHERE timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL @window MINUTE)
                    AND service_name = @service
                GROUP BY minute, service_name
                ORDER BY minute DESC
            """,
            'p99_latency': """
                SELECT 
                    APPROX_QUANTILES(latency_ms, 100)[OFFSET(99)] as p99_latency,
                    service_name
                FROM `{project}.{dataset}.{table}`
                WHERE timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL @window MINUTE)
                    AND service_name = @service
                GROUP BY service_name
            """
        }
    
    def build_safe_query(
        self, 
        metric_type: str, 
        service: str, 
        window_minutes: int = 60
    ) -> Tuple[str, Dict]:
        """æ§‹å»ºåƒæ•¸åŒ–æŸ¥è©¢ï¼Œé˜²æ­¢ SQL æ³¨å…¥"""
        
        # é©—è­‰è¼¸å…¥
        if metric_type not in self.query_templates:
            raise ValueError(f"Unknown metric type: {metric_type}")
        
        # åƒæ•¸åŒ–æŸ¥è©¢ï¼ˆå€Ÿé¡ bug assistant çš„å®‰å…¨å¯¦è¸ï¼‰
        query = self.query_templates[metric_type].format(
            project=os.getenv('GCP_PROJECT'),
            dataset='sre_metrics',
            table='service_metrics'
        )
        
        # åƒæ•¸å­—å…¸ï¼ˆé˜²æ­¢ SQL æ³¨å…¥ï¼‰
        params = {
            'window': window_minutes,
            'service': service
        }
        
        return query, params
```

## 3. Machine Learning Engineering - è¿­ä»£å„ªåŒ–æ¡†æ¶

### æ ¸å¿ƒåƒè€ƒæª”æ¡ˆ
- **å„ªåŒ–å¾ªç’°**: `docs/references/adk-samples-agents/machine-learning-engineering/machine_learning_engineering/sub_agents/optimization/agent.py`
- **è©•ä¼°ç³»çµ±**: `docs/references/adk-samples-agents/machine-learning-engineering/machine_learning_engineering/sub_agents/evaluation/agent.py`

### æ·±åº¦å€Ÿé¡åˆ†æ

#### A. SLO é…ç½®è¿­ä»£å„ªåŒ–
```python
# åŸºæ–¼ ML engineering çš„å„ªåŒ–å¾ªç’°æ”¹é€ 
class SLOIterativeOptimizer:
    """SLO é…ç½®çš„è¿­ä»£å„ªåŒ–å™¨"""
    
    def __init__(self):
        # å€Ÿé¡ ML çš„å¤šè¼ªå„ªåŒ–ç­–ç•¥
        self.optimization_rounds = 3
        self.improvement_threshold = 0.05  # 5% æ”¹é€²é–¾å€¼
        
    async def optimize_slo_config(
        self,
        current_config: Dict,
        historical_data: pd.DataFrame
    ) -> Dict:
        """è¿­ä»£å„ªåŒ– SLO é…ç½®"""
        
        best_config = current_config.copy()
        best_score = await self._evaluate_slo_config(best_config, historical_data)
        
        for round_num in range(self.optimization_rounds):
            # å€Ÿé¡ ML engineering çš„åƒæ•¸æœç´¢ç­–ç•¥
            candidates = self._generate_config_variants(best_config, round_num)
            
            # ä¸¦è¡Œè©•ä¼°ï¼ˆå¦‚ ML engineering çš„ä¸¦è¡Œè¨“ç·´ï¼‰
            scores = await asyncio.gather(*[
                self._evaluate_slo_config(config, historical_data)
                for config in candidates
            ])
            
            # é¸æ“‡æœ€ä½³é…ç½®
            for config, score in zip(candidates, scores):
                if score > best_score * (1 + self.improvement_threshold):
                    best_config = config
                    best_score = score
                    
                    # æ—©åœæ¢ä»¶ï¼ˆå€Ÿé¡ ML çš„æ—©åœæ©Ÿåˆ¶ï¼‰
                    if score > 0.95:  # 95% ç¬¦åˆç‡å·²è¶³å¤ å¥½
                        break
            
            # è¨˜éŒ„å„ªåŒ–æ­·ç¨‹ï¼ˆå¦‚ ML çš„è¨“ç·´æ—¥èªŒï¼‰
            await self._log_optimization_round(round_num, best_score, best_config)
        
        return best_config
    
    def _generate_config_variants(self, base_config: Dict, round_num: int) -> List[Dict]:
        """ç”Ÿæˆé…ç½®è®Šé«” - å€Ÿé¡ ML çš„è¶…åƒæ•¸æœç´¢"""
        variants = []
        
        # æ ¹æ“šè¼ªæ¬¡èª¿æ•´æœç´¢ç¯„åœï¼ˆé¡ä¼¼å­¸ç¿’ç‡è¡°æ¸›ï¼‰
        search_radius = 0.2 * (0.5 ** round_num)  # æŒ‡æ•¸è¡°æ¸›
        
        # èª¿æ•´å„å€‹ SLO åƒæ•¸
        for param in ['availability_target', 'latency_p99', 'error_budget']:
            for delta in [-search_radius, 0, search_radius]:
                variant = base_config.copy()
                variant[param] *= (1 + delta)
                variants.append(variant)
        
        return variants
```

## 4. HITL (Human-in-the-Loop) - å¯©æ‰¹æ©Ÿåˆ¶

### æ ¸å¿ƒåƒè€ƒæª”æ¡ˆ
- **HITL å¯¦ç¾**: `docs/references/adk-python-samples/human_in_loop/agent.py`
- **å¯©æ‰¹æµç¨‹**: `docs/references/adk-python-samples/human_in_loop/tools.py`

### æ·±åº¦å€Ÿé¡åˆ†æ

#### A. é«˜é¢¨éšªæ“ä½œå¯©æ‰¹ç³»çµ±
```python
# åŸºæ–¼ human_in_loop/tools.py æ”¹é€ 
class SREApprovalSystem:
    """SRE é«˜é¢¨éšªæ“ä½œå¯©æ‰¹ç³»çµ±"""
    
    def __init__(self):
        self.risk_matrix = {
            'restart_pod': {'dev': 'LOW', 'staging': 'MEDIUM', 'prod': 'HIGH'},
            'scale_deployment': {'dev': 'LOW', 'staging': 'MEDIUM', 'prod': 'HIGH'},
            'modify_config': {'dev': 'MEDIUM', 'staging': 'HIGH', 'prod': 'CRITICAL'},
            'delete_resource': {'dev': 'HIGH', 'staging': 'CRITICAL', 'prod': 'CRITICAL'}
        }
    
    async def request_approval(
        self,
        action: str,
        environment: str,
        context: Dict
    ) -> bool:
        """è«‹æ±‚äººå·¥å¯©æ‰¹ - å€Ÿé¡ HITL çš„ä¸­æ–·æ©Ÿåˆ¶"""
        
        risk_level = self.risk_matrix.get(action, {}).get(environment, 'CRITICAL')
        
        if risk_level in ['LOW']:
            # ä½é¢¨éšªè‡ªå‹•é€šé
            return True
        
        # æ§‹å»ºå¯©æ‰¹è«‹æ±‚ï¼ˆå€Ÿé¡ HITL çš„çµæ§‹åŒ–è«‹æ±‚ï¼‰
        approval_request = {
            'id': str(uuid.uuid4()),
            'timestamp': datetime.now().isoformat(),
            'action': action,
            'environment': environment,
            'risk_level': risk_level,
            'context': context,
            'requester': context.get('agent_id', 'sre_assistant'),
            'timeout': 300 if risk_level == 'CRITICAL' else 600  # ç§’
        }
        
        # ç™¼é€åˆ°å¯©æ‰¹éšŠåˆ—ï¼ˆå¦‚ HITL çš„ credential storeï¼‰
        await self._send_to_approval_queue(approval_request)
        
        # ç­‰å¾…å¯©æ‰¹ï¼ˆå€Ÿé¡ HITL çš„è¶…æ™‚æ©Ÿåˆ¶ï¼‰
        try:
            result = await asyncio.wait_for(
                self._wait_for_approval(approval_request['id']),
                timeout=approval_request['timeout']
            )
            return result
        except asyncio.TimeoutError:
            # è¶…æ™‚è‡ªå‹•æ‹’çµ•ï¼ˆå®‰å…¨å„ªå…ˆï¼‰
            await self._log_timeout(approval_request)
            return False
```

## 5. é—œéµå¯¦ç¾å„ªå…ˆç´šå»ºè­°

åŸºæ–¼ä»¥ä¸Šæ·±åº¦åˆ†æï¼ŒSRE Assistant çš„å¯¦ç¾å„ªå…ˆç´šï¼š

### P0 - ç«‹å³å¯¦æ–½ï¼ˆå¾é€™äº›ç¯„ä¾‹ç›´æ¥ç§»æ¤ï¼‰
1. **RAG å¼•ç”¨ç³»çµ±** (`RAG/agent.py`, `RAG/tools.py`)
   - ç›´æ¥ä½¿ç”¨å…¶å¼•ç”¨æ ¼å¼åŒ–é‚è¼¯
   - æ¡ç”¨å¤šéšæ®µæª¢ç´¢ç­–ç•¥

2. **GitHub Issue æ¨¡æ¿** (`software-bug-assistant/tools/github_tools.py`)
   - è¤‡è£½å…¶ Markdown æ¨¡æ¿çµæ§‹
   - ä½¿ç”¨ç›¸åŒçš„æ¨™ç±¤ç³»çµ±

3. **HITL å¯©æ‰¹æµç¨‹** (`human_in_loop/tools.py`)
   - ç§»æ¤é¢¨éšªè©•ä¼°çŸ©é™£
   - æ¡ç”¨ç›¸åŒçš„è¶…æ™‚æ©Ÿåˆ¶

### P1 - çŸ­æœŸæ•´åˆï¼ˆéœ€è¦é©é…ï¼‰
1. **MCP è³‡æ–™åº«å·¥å…·** (`software-bug-assistant/tools/database_tools.py`)
   - æ”¹é€ æŸ¥è©¢æ¨¡æ¿ç‚º Prometheus/BigQuery
   - ä¿ç•™åƒæ•¸åŒ–æŸ¥è©¢æ¨¡å¼

2. **è¿­ä»£å„ªåŒ–æ¡†æ¶** (`machine-learning-engineering/.../optimization/`)
   - å°‡ ML æ¨¡å‹å„ªåŒ–æ”¹ç‚º SLO é…ç½®å„ªåŒ–
   - ä¿ç•™ä¸¦è¡Œè©•ä¼°æ©Ÿåˆ¶

### P2 - é•·æœŸå€Ÿé¡ï¼ˆæ¦‚å¿µé·ç§»ï¼‰
1. **å¤šä»£ç†å”ä½œ** (FOMC Research çš„å”èª¿æ¨¡å¼)
2. **è¦–è¦ºåŒ–å ±å‘Š** (Data Science çš„åœ–è¡¨ç”Ÿæˆ)
3. **Web ç’°å¢ƒäº’å‹•** (Personalized Shopping çš„å°èˆªæ¨¡å¼)